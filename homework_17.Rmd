---
title: "Homework 17"
author: "Nathan Kurtz-Enko"
date: "4/19/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(class) ##for knn
library(e1071) ##for svm
library(gridExtra) ## for grid.arrange
library(randomForest)
```

#Assignment 1: BUPA Liver data
Apply Support Vector Machines to BUPA Liver data
https://archive.ics.uci.edu/ml/datasets/Liver+Disorders
#Attribute information:
1. mcv	mean corpuscular volume
2. alkphos	alkaline phosphotase
3. sgpt	alamine aminotransferase
4. sgot 	aspartate aminotransferase
5. gammagt	gamma-glutamyl transpeptidase
6. drinks	number of half-pint equivalents of alcoholic beverages
drunk per day
7. selector (field used to split data into two sets)

*Goal* Build the best possible SVM to classify a modified Field 6 based on Fields 1-5. 
Make this a classification scenario. Create a new variable "HighAlcohol" to indicate if drinks>=1
(you can change this). Drop the drinks variable. 

You can use the selector field to make a train/test combination.
Use these to estimate the error rates for various modeling approaches.
Compare SVM, with different kernels (linear, poly, and radial) 
to some other classification tools, e.g, logistic, lasso, knn,
randomForest, boosting.  


##Get things started. 
```{r}
liver.df <-
  read.csv("/home/rstudio/users/kurtze1/ADM/homework/bupa.data")
names(liver.df) <- c("mcv","alkphos","sgpt","sgot","gannagt","drinks","selector")
names(liver.df)
liver.df <- liver.df %>% 
  mutate(HighAlcohol=drinks>=1) %>% 
  dplyr::select(-drinks)

validate.df <- liver.df %>% 
  filter(selector==2) %>% 
  dplyr::select(-selector)
nrow(validate.df)

liver.df <- liver.df %>% 
  filter(selector!=2) %>% 
  dplyr::select(-selector)
nrow(liver.df)  

with(liver.df,table(HighAlcohol))


```

```{r}

theCost <- 10
costVals <- 10^seq(-2,2,by=1)
gammaVals <- 2^seq(-1,3)
degreeVals <- 1:4

liver.X <- liver.df[, -length(liver.df)]
liver.Y <- liver.df[, length(liver.df)]
validate.X <- validate.df[, -length(validate.df)]
validate.Y <- validate.df[, length(validate.df)]
```

```{r, warning=FALSE}

svm.tune.lin <- tune(svm,factor(HighAlcohol)~.,
     data=liver.df,
     kernel="linear",
     ranges=list(cost=costVals),
     tunecontrol=tune.control(cross=5)
     )
svm.tune.poly <- tune(svm,factor(HighAlcohol)~.,
     data=liver.df,
     kernel="poly",
     ranges=list(cost=costVals,
                 degree=degreeVals),
     tunecontrol=tune.control(cross=5)
     )
svm.tune.rad <- tune(svm,factor(HighAlcohol)~.,
     data=liver.df,
     kernel="radial",
     ranges=list(cost=costVals,
                 gamma=gammaVals),
     tunecontrol=tune.control(cross=5)
     )

(cost.best.lin <- svm.tune.lin$best.parameters$cost)
(cost.best.poly <- svm.tune.poly$best.parameters$cost)
(degree.best.poly <- svm.tune.poly$best.parameters$degree)
(cost.best.rad <- svm.tune.rad$best.parameters$cost)
(gamma.best.rad <- svm.tune.rad$best.parameters$gamma)

mod.svm.lin <- svm(factor(HighAlcohol)~.,
               data=liver.df,
               kernel="linear",
               cost=cost.best.lin,
               scale=F)
mod.svm.poly <- svm(factor(HighAlcohol)~.,
               data=liver.df,
               kernel="poly",
               cost=cost.best.poly,
               degree = degree.best.poly,
               scale=F)
mod.svm.rad <- svm(factor(HighAlcohol)~.,
               data=liver.df,
               kernel="radial",
               cost=cost.best.rad,
               gamma = gamma.best.rad,
               scale=F)

pred.lin <- predict(mod.svm.lin, newdata = validate.df)
pred.poly <- predict(mod.svm.poly, newdata = validate.df)
pred.rad <- predict(mod.svm.rad, newdata = validate.df)

err.lin <- mean(pred.lin != validate.Y)
err.poly <- mean(pred.poly != validate.Y)
err.rad <- mean(pred.rad != validate.Y)
```

```{r}


kvals <- 1:30
cv_errs <- tibble(k = kvals, err = kvals)

for(k in kvals){
  mod.knn <- knn.cv(liver.X, liver.Y, k = k)
  err <- mean(liver.Y != mod.knn)
  cv_errs[k, 2] = err
}
```

```{r}
index <- which.min(cv_errs$err)
best_k <- as.numeric(cv_errs[index, 1])
```

```{r}
mod.knn <- knn(liver.X, validate.X, liver.Y, k = best_k)
err.knn <- mean(mod.knn != validate.Y)
```


```{r}
c(err.knn, err.lin, err.poly, err.rad)
```



#Assignment 2: Spam Revisited.
The last time we looked at the spam data set. How does SVM compete with RandomForest
and boosting as a spam predictor. Since you have some much spam data, use a train/test
combination to evaluate your error rate.
Make sure all your models are optimized before 
drawing any conclusions.
Note: Make sure your spam response variable is a factor.
That is


    
```{r}
spam.df <- read_csv("/home/rstudio/users/kurtze1/ADM/class/SPAM.csv")

spam.df <- spam.df %>% 
  mutate(IsSpam=factor(IsSpam))%>%
  dplyr::select(-`3d`)

n <- nrow(spam.df)
sample <- sample(1:n, n/2, replace = FALSE)
validate2.df <- spam.df[-sample, ]
spam.df <- spam.df[sample, ]

```

```{r}

theCost <- 10
costVals <- 10^seq(-2,2,by=1)
gammaVals <- 2^seq(-1,3)
degreeVals <- 1:4

spam.X <- spam.df[, -1]
spam.Y <- spam.df[, 1]
validate2.X <- validate2.df[, -1]
validate2.Y <- validate2.df[, 1]
```

```{r, warning=FALSE}

svm.tune.lin <- tune(svm, IsSpam~.,
     data=spam.df,
     kernel="linear",
     ranges=list(cost=costVals),
     tunecontrol=tune.control(cross=5)
     )
svm.tune.poly <- tune(svm,IsSpam~.,
     data=spam.df,
     kernel="poly",
     ranges=list(cost=costVals,
                 degree=degreeVals),
     tunecontrol=tune.control(cross=5)
     )
svm.tune.rad <- tune(svm,IsSpam~.,
     data=spam.df,
     kernel="radial",
     ranges=list(cost=costVals,
                 gamma=gammaVals),
     tunecontrol=tune.control(cross=5)
     )

(cost.best.lin <- svm.tune.lin$best.parameters$cost)
(cost.best.poly <- svm.tune.poly$best.parameters$cost)
(degree.best.poly <- svm.tune.poly$best.parameters$degree)
(cost.best.rad <- svm.tune.rad$best.parameters$cost)
(gamma.best.rad <- svm.tune.rad$best.parameters$gamma)

mod.svm.lin <- svm(factor(IsSpam)~.,
               data=spam.df,
               kernel="linear",
               cost=cost.best.lin,
               scale=F)
mod.svm.poly <- svm(factor(IsSpam)~.,
               data=spam.df,
               kernel="poly",
               cost=cost.best.poly,
               degree = degree.best.poly,
               scale=F)
mod.svm.rad <- svm(factor(IsSpam)~.,
               data=spam.df,
               kernel="radial",
               cost=cost.best.rad,
               gamma = gamma.best.rad,
               scale=F)

pred.lin <- predict(mod.svm.lin, newdata = validate2.df)
pred.poly <- predict(mod.svm.poly, newdata = validate2.df)
pred.rad <- predict(mod.svm.rad, newdata = validate2.df)

err.lin <- mean(pred.lin != validate2.df$IsSpam)
err.poly <- mean(pred.poly !=   validate2.df$IsSpam)
err.rad <- mean(pred.rad != validate2.df$IsSpam)
```

```{r, eval = FALSE}

##We need to note the number of predictors
p <- ncol(spam.df)-1

#random forest stuff
spam.rf <- randomForest(IsSpam ~ `3d`, data = spam.df, ntree = 10, mtry = sqrt(p), importance = TRUE) ## 100 trees

pred <- predict(spam.rf, newdata = validate2.df, type = "class")

rf_mse <- mean((pred != validate2.df$IsSpam))
```

```{r}
c(err.lin, err.poly, err.rad)
```

