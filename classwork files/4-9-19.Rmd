---
title: "4-9-19"
author: "Nathan Kurtz-Enko"
date: "4/9/2019"
output: pdf_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,cache = TRUE)
```
#Boosting
The idea behind boosting is that we employ "weak learning" over and
over again in order to get a suitable approximation to the data.
In what follows, the weak learner will involve small trees,
depth =1 (stumps) or depth=2


Libraries
```{r}
library(ggplot2)
library(tidyverse)
library(tree)
```


Include a different tree package.
```{r}
library(rpart)
```



##Boosting by hand
Create a synthetic dataset builder.
```{r}
buildData<-function(n,sig=.5,S=.1){
  x<-runif(n,0,2)
  y<-x*(1-x)+sin(10*x)
  y1<-S*dnorm(x,1,.02)
  y2<-S*dnorm(x,0.5,.02)
  y3<-S*dnorm(x,.2,.02)
  y4<-S*dnorm(x,1.3,.02)
  y5<-S*dnorm(x,1.8,.02)
  dat<-y+y1-y2+y3-y4+y5+rnorm(n,0,sig)
  data.frame(x,y=dat) 
}
```

Training data
```{r}
n <- 200
dataTrain.df <-buildData(n) %>% 
  mutate(approx=0,
           ## Our initial theBoostiduals...same as our target
           residual=y)
```
What are we looking at
```{r}
dataTrain.df %>% 
  ggplot()+
  geom_point(aes(x,y),color="black")
```


We will keep looking at this picture. It has the current approximation,
the target function, and the residuals
Initially..
```{r}
dataTrain.df %>%
  gather(type,val,y:residual) %>%
  ggplot(aes(x,val,color=type))+
  scale_color_manual(values=c("red","blue","black"))+
  geom_point()

```



##Simple weak learning...use a simple tree to estimate y

Fix the depth and the shrinkage factors
```{r}
depth <- 1
lambda <- .1
```


Build a tree (stump in this case)..Note, using rpart, instead of tree.
```{r}
tree0 <- rpart(residual~x,
               data=dataTrain.df,
               control=rpart.control(maxdepth=depth))

```


Look at the tree
```{r}
plot(tree0)
text(tree0,pretty=0,cex=1)

```


Make predictions
Slice off just a little bit...
compute the residuals and current approximation
```{r}
preds <- as.numeric(predict(tree0))
dataTrain.df <- dataTrain.df%>%
  ##only use a small about of the residuals..."sanding"
    mutate(residual=residual-lambda*preds,
           ##update the approximation
           approx=approx+lambda*preds)

```


The state of affairs right now
```{r}
dataTrain.df %>%
  gather(type,val,y:residual) %>%
  ggplot(aes(x,val,color=type))+
  scale_color_manual(values=c("red","blue","black"))+
  geom_point()
```


We see a smaller residue and (ever so) slightly better approximation. This is
a weak learner.


Make the boosting a function of the data, lambda, and the depth of
the tree.
Note that his function returns a list consisting of the new data
frame and the model that was used.
```{r}
doBoost<-function(data.df,lambda,depth){
  ##Build a simple tree
    tree0 <- rpart(residual~x,
                   data=data.df,
                   control=rpart.control(maxdepth=depth))
    ##make the predictions
    preds <- as.numeric(predict(tree0))
    ##update the data
    data.df <- data.df %>%
        mutate(residual=residual-lambda*preds,
               approx=approx+lambda*preds)
    ##return the data and the model
    list(data=data.df,tree=tree0)
}

```


##One step of boosting
Set up the variables
```{r}
lambda<-.1
depth<-2
data.df <- dataTrain.df

##the result of the boosting
theBoost<-doBoost(data.df,lambda,depth)
```

Pull off the data and the tree that was used
```{r}

data.df <- theBoost[["data"]]
tree0 <- theBoost[["tree"]]
```


Plot the data..again, a slightly better approximation with slightly smaller residuals
```{r}
data.df %>%
    gather(type,val,y:residual) %>%
    ggplot(aes(x,val,color=type))+
    scale_size_manual(values=c(2,2,.25))+
    scale_color_manual(values=c("red","blue","black"))+
    geom_point()

```



Boost a number of times
At each step, keep track of the new data frame and the model producted
The choice of lambda is important. Here, we'll just use an arbitrary starting value. In general, this is something that is determined by cross-validation.

Boost numBoost times. Keep track of all the trees that are built along the way. These are the resulting model used for prediction. Use a list (not an array) to store these.
```{r}
lambda<-.1
modelList <- list()
numBoost <- 1000
depth<-2
for(i in 1:numBoost){
    theBoost<-doBoost(data.df,lambda,depth)
    data.df <- theBoost[["data"]]
    modelList[[i]]<- theBoost[["tree"]]
   ## print(i)
}
```


....and now look at a plot of the data
```{r}
data.df %>%
    gather(type,val,y:residual) %>%
    ggplot(aes(x,val,color=type))+
    scale_size_manual(values=c(2,2,.25))+
    scale_color_manual(values=c("red","blue","black"))+
    geom_point()


```


The last Model
```{r}
modelList[[numBoost]]

depth
yy<-data.df$residual
xx<-data.df$x
plot(xx,yy)
rpart(yy~xx,control=rpart.control(maxdepth=2))

```

The tree is just a root at this point...no further effect.  This
happens with a simple data set, rarely see this in practice with a
a real data set.

Check that there isn't a good split on the residuals now
```{r}
xMin <- with(data.df,min(x))
xMax <- with(data.df,max(x))
steps <- 50
mse<-numeric(steps)
i<-1
for(x0 in seq(xMin,xMax,length=steps)){
  mse[i]<-data.df %>%
    mutate(class=x<x0) %>%
    group_by(class) %>%
    mutate(residual.hat = mean(residual)) %>%
    mutate( r=mean((residual-residual.hat)^2)) %>%
    with(sum(r))
  i<-i+1
}
plot(mse)
```


Not much change
```{r}
(max(mse)-min(mse))/mean(mse)

```



##Make a prediction with these boosted trees
The idea is to run a prediction through all the weak learners, take
the sum of the weak predictions

Helper function
```{r}
predictFromModels <- function(aModel,someData){
    predict(aModel, newdata=someData)
}
```

Test it out...it makes pedictions from a tree.
```{r}
preds <- predictFromModels(modelList[[1]],dataTrain.df)
head(preds)
```


Here we go. To make predictions from a boosted model, compute the predictions from each of the trees. Then we combine the predictions (scaled by lambda).
```{r}
maxMod <- numBoost
##sapply works best here
predsAll <- sapply(1:maxMod,function(i) predictFromModels(modelList[[i]],dataTrain.df))
```

Now convert to a matrix, one row for each model, one column for each observation
```{r}
predsAll.mat<- matrix(predsAll,nrow=maxMod,byrow=T)
```

Compute the weighted sum of each observation (column)
```{r}
predsAll.pred <- lambda*colSums(predsAll.mat)
```

Put these into the data frame as the prediction. 
```{r}
dataTrain.df$pred <- predsAll.pred
```


The plot of the data and the prediction
```{r}
dataTrain.df%>%
    dplyr::select(-approx,-residual)%>%
    gather(type,val,y:pred)%>%
    ggplot()+
    scale_color_manual(values=c("red","blue","black"))+
    geom_point(aes(x,val,color=type))
```



## Make predictions on the test data.
Build some test data

```{r}
dataTest.df <- buildData(400)

```



Prediction on the new data
```{r}
predsAll <- sapply(1:maxMod,function(i) predictFromModels(modelList[[i]],dataTest.df))
predsAll.mat<- matrix(predsAll,nrow=maxMod,byrow=T)
predsAll.pred <- colSums(predsAll.mat)*lambda
dataTest.df$pred.boost <- predsAll.pred
```

How's it look?
```{r}
head(dataTest.df)
dataTest.df%>%
    gather(type,val,y:pred.boost)%>%
    ggplot()+
    scale_color_manual(values=c("red","blue","black"))+
    geom_point(aes(x,val,color=type))
```


The MSE
```{r}
(mse.boost <- with(dataTest.df,mean((y-pred.boost)^2)))

```

Compared to what?? How about a linear model?
```{r}
mod.lm <- lm(y~x,data=dataTrain.df)
dataTest.df$pred.lm <- predict(mod.lm,newdata=dataTest.df)
(mse.lm <- with(dataTest.df,mean((y-pred.lm)^2)))
```


A visual comparison
```{r}
dataTest.df%>%
    gather(type,val,y:pred.lm)%>%
    ggplot()+
    scale_color_manual(values=c("red","blue","black"))+
    geom_point(aes(x,val,color=type))


```


How about Random Forest?
```{r}
library(randomForest)
mod.rf <- randomForest(y~x,data=data.df)
dataTest.df$pred.rf <- predict(mod.rf,newdata=dataTest.df,
                ntree=1000)


```

```{r}
dataTest.df%>%
    gather(type,val,y:pred.rf)%>%
    ggplot()+
    scale_color_manual(values=c("red","blue","black","orange"))+
    geom_point(aes(x,val,color=type))
```

Compare the three different models
```{r}
mse.rf <- with(dataTest.df,mean((y-pred.rf)^2))
c(mse.boost,mse.lm,mse.rf)
```

Random Forest wins, in this case. 

##Using the gradient boosting  library.
Load in gbm
```{r}
library(gbm)
```

Use the Boosting package.
```{r}
maxTrees <- 5000
mod.gbm <- gbm(y~x,
               data=dataTrain.df,
               n.trees=maxTrees,
               shrinkage=.01,
               distribution="gaussian",
               interaction.depth = 2)

```


MSE on train and test data
```{r}
numTrees <- 5000
pred <- predict(mod.gbm,n.trees=numTrees,newdata=dataTrain.df)
dataTrain.df$pred.gbm <- pred
with(dataTrain.df,mean((y-pred.gbm)^2))
```

Now the testing data.
```{r}
pred <- predict(mod.gbm,n.trees=numTrees,newdata=dataTest.df)
dataTest.df$pred.gbm <- pred
```

How are doing?
```{r}
(mse.gbm <- with(dataTest.df,mean((y-pred.gbm)^2)))
c(mse.boost,mse.lm,mse.rf,mse.gbm)
``` 


The number of weak learners and  the value of lambda are  control parameters that need to be optmized...you can overfit a boosted model.

```{r}
K <- 1000
mult <- numTrees/K
mse <-matrix(nrow=K,ncol=2)
for(k in 1:K){
    predTest <- predict(mod.gbm,n.trees=k*mult,newdata=dataTest.df)
    predTrain <- predict(mod.gbm,n.trees=k*mult,newdata=dataTrain.df)
    mse[k,1] <- with(dataTrain.df,mean((y-predTrain)^2))
    mse[k,2] <- with(dataTest.df,mean((y-predTest)^2))
}

data.frame(trees=1:K,mseTrain=mse[,1],mseTest=mse[,2])%>%
    gather(type,val,mseTrain:mseTest)%>%
    ggplot()+
    scale_x_continuous(breaks=seq(0,K,length=11),labels=seq(0,numTrees,length=11))+
    scale_color_manual(values=c("red","blue","black"))+
    geom_line(aes(trees,val,color=factor(type)))+
    ggtitle("GBM MSE for Train and Test")

```


IYou can see that the mse for the Test data is (perhaps) starting to increase
as the number of trees increases...gbm can overfit.



## Simple test case...
How does GBM compare with a linear model?


Use a simple quadratic function as the underlying model
```{r}
f <- function(x) 5*x*(1-x)
buildData<-function(n,sig=.5){
  x <- runif(n,0,1)
  y <- f(x)+rnorm(n,0,sig)
  data.frame(x,y)
}
```


Generate some data
```{r}
train1.df <-  buildData(100)
```


Take a look
```{r}
ggplot(train1.df)+
    geom_point(aes(x,y),color="blue")


```


Build a linear model with a quadratic term
```{r}
mod.lm <- lm(y~1+x+I(x^2),data=train1.df)
train1.df$pred.lm <- predict(mod.lm)
train1.df%>%
    gather(type,val,y:pred.lm)%>%
    ggplot()+
    geom_point(aes(x,val,color=type))+
    scale_color_manual(values=c("black","red","blue"))


```


Build a boosted model with a large number of trees and (relatively)
small shrinkage factor.
```{r}
mod.gbm <- gbm(y~x,data=train1.df,
               n.trees=5000,
                distribution="gaussian",
               interaction.depth = 2,
                shrinkage=0.1)

```


Prediction with the boosted model
```{r}
train1.df$pred.gbm <- predict(mod.gbm,n.trees=5000)
train1.df%>%
    gather(type,val,y:pred.gbm)%>%
    ggplot()+
    geom_point(aes(x,val,color=type))+
    scale_color_manual(values=c("orange","red","blue"))


```


GBM wins on the training data
```{r}
with(train1.df,mean((y-pred.lm)^2))
with(train1.df,mean((y-pred.gbm)^2))

```



Create some Test data
```{r}

test1.df <- buildData(100)
```


GBM loses here
```{r}
test1.df$pred.lm <- predict(mod.lm,newdata=test1.df)
test1.df$pred.gbm <- predict(mod.gbm,newdata=test1.df,n.trees=1000)

```


Much closer on test data.
```{r}
test1.df%>%
    gather(type,val,y:pred.gbm)%>%
    ggplot()+
    geom_point(aes(x,val,color=type))+
    scale_color_manual(values=c("orange","red","blue"))+
    ggtitle("Linear vs GBM on test data")

```

```{r}
with(test1.df,mean((y-pred.lm)^2))
with(test1.df,mean((y-pred.gbm)^2))
```






#Cross validationg to select Boosting control parameters
In boosting, in general, and GBM, in particular, there are three distinct
control parameters

* The interaction depth (sometimes, the number of terminal leaves)
* The shrinkage factor (also called the learning rate)
* The number of trees

As you can imagine, we use cross-validation to estimate the
optimal value for each of these parameter


# Assignment: Spam data
Compared with random forests, can you improve the error rate estimates for the Spam? You will have to explore values for the all the control parameters.

Here's a start, only using training data.
```{r} 
spam.df <- read.csv("SPAM.csv")
```
Build a model, in this case we are doing classification, so the distribution is "bernoulli".

The control parameters are arbitrary.
```{r}
numTrees <- 100
depth <- 7
lambda <- .1
spam.gbm <- gbm(IsSpam ~ ., data=spam.df,
                n.trees=numTrees,
                distribution="bernoulli",
                interaction.depth = depth,
                shrinkage=lambda)


```


There is variable importance with Boosted trees.  The summary function produces some information and a plot
```{r}
spam.gbm$var.names
summary(spam.gbm,cBars=54)
```
Documentation is available at
https://www.rdocumentation.org/packages/gbm/versions/2.1.5/topics/summary.gbm

### Assignment: Build a better variable importance plot.


Note: Ada Boost is another option when doing a classification. We will talk about Ada boosting next time.
```{r}
spam.gbm.ada <- gbm(IsSpam ~ ., data=spam.df,
                n.trees=numTrees,
                distribution="adaboost",
                interaction.depth =depth,
                shrinkage=lambda)
```

```{r}
probs <- predict(spam.gbm,newdata=spam.df,n.trees=100,type="response")
##convert to a prediction.
preds <-  probs > 0.5

## How does it look?
with(spam.df,table(IsSpam,preds))
with(spam.df,mean(IsSpam != preds))
```


A quick look at the error as a function of the number of trees.
```{r}
errTree <- numeric(numTrees)
for(n in 1:numTrees){
  probs <- predict(spam.gbm,newdata=spam.df,n.trees=n,type="response")
  preds <-  probs > 0.5
  errTree[n] <- with(spam.df,mean(IsSpam != preds))  
}


data.frame(trees=1:numTrees,err=errTree) %>% 
  ggplot()+
  geom_point(aes(trees,err))

```

### Assignment: 
Extend this analysis using cross validation on the spam data. To start, fix the interaction depth at 2. Cross validate over the shrinkage and the number of trees. 

Once this works, repeat with interaction depths of 1, 3, 4,.... What is your best estimate of the error rate using Boosting on this data set?

```{r}
summary(spam.gbm)
```


#Assignment: ALS Data set
Consider the ALS dataset (again) from CASI.

https://web.stanford.edu/~hastie/CASI/data.html

Reproduce Figures 17.7, 17.8, and 17.10

