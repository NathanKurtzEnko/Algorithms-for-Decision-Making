---
title: "3-5-19"
author: "Nathan Kurtz-Enko"
date: "3/5/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,cache = TRUE)
```


There are:
*10000 records
*3 predictor variables: student (factor), balance and income (num)
*1 response: default (factor "Yes/No")


This is useful...if there are too many variables, it can get
confusing

```{r,message=F}
library(GGally)
ggpairs(iris)
```

Create a numerical response variable. This can be useful later

```{r}
Default <- mutate(Default,
                  default.val=ifelse(default=="Yes",1,0))


```

A plot of balance and income, shape indicates student or not.

```{r}
Default %>% 
  ggplot()+
  geom_point(aes(balance,income,
                 color=default,shape=student),
             size=2,alpha=0.75)+
  scale_color_brewer(palette="Set1")+
  ggtitle("Default by balance and income")

```

Break out students using a facet_wrap
```{r}
Default %>% 
  mutate(student=ifelse(student=="Yes",
                        "Student","NonStudent")) %>% 
  ggplot()+
    geom_point(aes(balance,income, 
                   color=default),
               size=1,alpha=0.5)+
    facet_wrap(~student,ncol=1)+
    scale_color_brewer(palette="Set1")+
    ggtitle("Default by balance and income")

```

Looks like balance is worth using as a starter predictor.

```{r}
Default %>% 
  ggplot()+
      geom_point(aes(balance,default.val),
                 color="blue")+
      ggtitle("Default by balance")
```

```{r}
ggplot(Default)+
    geom_jitter(aes(balance,default.val),
                height=.05,size=.1,color="blue")+
    ggtitle("Default by balance")
```
Or, consider just income

```{r}
Default %>% 
  ggplot()+
    geom_jitter(aes(income,default.val),
                height=.1,size=.1,color="blue")+
    ggtitle("Default by income")    


```
# Build a linear model using balance
This is not the best way to proceed. Compare to Figure 4.2

```{r}
mod.lm<-lm(default.val~balance,data=Default)
summary(mod.lm)

```
Assign the predicted outcomes
```{r}
Default$val.lm<-predict(mod.lm,newdata=Default)


```

The picture....

```{r}
Default %>% 
  ggplot()+
  geom_jitter(aes(balance,default.val),
              height=.1,
              size=.1,color="blue")+
  geom_point(aes(balance,val.lm),
             size=.1,
             color="red")+
  scale_y_continuous(breaks=seq(0,1,by=.1))+
  ggtitle("Linear model to predict default")

```

This works poorly. But we can  still build a classifer. 

Use a rule: if pred.lm < threshold, classify as no, otherwise yes.

Question: What is the best threshold?
Answer: test a bunch of values.

```{r}
thresh.pred<-0.01
Default<-Default %>% 
  mutate(pred.lm= ifelse(val.lm < thresh.pred,0,1))
```

Confusion matrix and error rate
```{r}
with(Default,table(default.val, pred.lm))
(err.pred<-with(Default,mean(default.val!=pred.lm)))
```

```{r}
calcErr <- function(thresh.pred) {
 Default<-Default %>% 
  mutate(pred.lm = ifelse(val.lm < thresh.pred,
                               0,1))
  with(Default,mean(default.val != pred.lm))
}
calcErr(0.01)
calcErr(0.9)
```

```{r}

threshVals<-seq(0,1,length.out = 100)
err.preds<-map_dbl(threshVals,calcErr)


data.frame(thresh=threshVals,err=err.preds) %>% 
  ggplot()+
  geom_point(aes(thresh,err),size=0.5)

```

Track down the min...
```{r}
min(err.preds)
id.min<-which.min(err.preds)
(thresh.best<-threshVals[id.min])
```

How about the Null Rate?
```{r}
with(Default,table(default))
(err.null<-with(Default,mean(default.val != 0)))
```

Always predict No, then the error rate is 3%! We can slightly beat this.


Train+test

```{r}
(n<-nrow(Default))
train<-sample(1:n,n/2,rep=F)
train.df<-Default[train,]
test.df<-Default[-train,]
```

```{r}
mod.lm<-lm(default.val~balance,data=train.df)
test.df$val.lm<-predict(mod.lm,newdata=test.df)

test.df<-test.df %>% 
  mutate(pred.lm=ifelse(val.lm < thresh.best,0,1))

with(test.df,table(default.val,pred.lm))
(err.lm2<-with(test.df,mean(default.val!=pred.lm)))
```

Still better than the null rate, not by much.

# Better: Logistic Regression Model

The probability of going from 0 to 1 is clearly not linear in the value of balance. 

Here's a rough look at the trend, based on binning values.
```{r}
qnts<-with(Default,quantile(balance,seq(0,1,by=.05)))
brks<-qnts[1:(length(qnts)-1)]
Default$cut<-with(Default,cut(balance,breaks=qnts,
              labels=brks))
Default<-Default %>% 
  mutate(cut=as.numeric(as.character(cut)))
with(Default,table(cut))

Default %>% 
  group_by(cut) %>% 
  summarize(p=mean(default.val)) %>% 
  ggplot()+
  geom_point(aes(cut,p),color="blue",size=2)+
   geom_hline(yintercept = 1,color="black")+
  geom_hline(yintercept = 0,color="black")+
  geom_segment(aes(x=cut,xend=cut,y=0,yend=p),color="blue")+
  scale_x_continuous(breaks=seq(0,3000,length.out = 6))+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

Logistic Regression attempts to model the probability more realistically.

This is the standard syntax.

```{r}
mod.log <- glm(default~balance,
               family=binomial,
               data=Default)
summary(mod.log)
```

Pull off the predicted probabilities
Note the type="response"
The default is to return the log odds

```{r}
probs <- predict(mod.log,data=Default,type="response")
head(probs)
```

**ALTERNATIVE**...
predict.glm has different types of return values
a different way to think of this....
The default is to return the log odds values

```{r}
log.odds<- predict(mod.log,data=Default)
```
Convert log odds to probability
```{r}
probs0<- exp(log.odds)/(1+exp(log.odds))
head(cbind(probs,probs0))
```
Same as above.

Assign predicted **probabilities**
```{r}
Default <- mutate(Default,prob.log = probs)
```

Figure 4.2 again..this time with logistic regression predicting the probabilty of default.
```{r}
names(Default)
Default %>% 
  ggplot()+
  geom_jitter(aes(balance,default.val),
              height=.1,size=.1,color="blue")+
  geom_point(aes(balance,prob.log),color="red")+
  geom_hline(yintercept = 1,color="black")+
  geom_hline(yintercept = 0,color="black")+
  ggtitle("Default by balance") 

```

##How well did we do: Error Rates and Null Error Rates
Let's do some predicting.....
set up a probability threshold for classifying as a "Yes."As a start, 0.5 is a natural choice


```{r}
thresh.pred <-.5
Default<-Default %>% 
  mutate(pred.log= ifelse(prob.log < thresh.pred ,0,1))
```
The Confusion matrix..cross tabulate actual vs predicted
```{r}
with(Default,table(default.val, pred.log))
```
We see
*9625 No's correctly classified  (Sensitivity)
*42 False Positives
*233 False Negatives
*100 Yes's correctly classified (Specificity)

error rate
```{r}
(err.log <- with(Default, mean(default.val != pred.log)))
```
An error rate of 2.75%. Not bad

Better than the null rate.
Compared to the null rate, we did do better, by 1% in absolute terms
```{r}
c(err.log,err.null)
```

Now, let's group by students and see how we did in each class.
It looks as if students are harder to predict!

```{r}
Default %>%
    group_by(student)%>%
    summarize(err=mean(!default.val == pred.log),
              err.null=mean(default=="Yes"),
              err.ratio=err/err.null)

```

## Best threshold

Build the calcErr function
```{r}

calcErr <- function(thresh.default) {
 Default<-Default %>% 
  mutate(default.pred= ifelse(prob.log< thresh.default,0,1))
  with(Default,mean(default.val!=default.pred))
}
  
```

```{r}
threshVals<-seq(0,0.5,length.out = 100)
err.preds<-map_dbl(threshVals,calcErr)
plot(threshVals,err.preds)

```
Look for the min value.
```{r}
min(err.preds)
id.min<-which.min(err.preds)
threshVals[id.min]
```

At this stage, we aren't doing much better than we did with the simple linear model. As well, we're not doing any traing+test comparisons!


#Logistic Regression with multiple predictors
Add another predictor...income

There appears to be something going on??

```{r}
plot1.gg <- ggplot(Default,aes(balance,income, shape=default,color=default))+
    geom_point(size=2,alpha=0.5)+
    scale_color_brewer(palette="Set1")+
    ggtitle("Default by balance and income")
plot1.gg


```

##The Model
```{r}
mod.log2 <- glm(default~balance+income,
                family=binomial,
                data=Default)
summary(mod.log2)

```
Assign probabilities
```{r}
probs <- predict(mod.log2,data=Default,type="response")
Default <- mutate(Default,prob.log = probs)

```

How does this look on students vs non-students?
How would we interpret this?

```{r}
ggplot(Default)+
    geom_jitter(aes(balance,default.val),height=.1,size=.2,
                color="blue")+
    geom_jitter(aes(balance,prob.log,color=student),
                height=.02,size=.2)+
    scale_color_brewer(palette="Set1")

```

Did we do any better?

```{r}
threshVals<-seq(0,0.5,length.out = 100)
err.preds<-map_dbl(threshVals,calcErr)
plot(threshVals,err.preds)

min(err.preds)
id.min<-which.min(err.preds)
threshVals[id.min]
```

Confusion matrix
```{r}
Default<-Default %>% 
  mutate(pred.log=ifelse(prob.log<threshVals[id.min],0,1))
with(Default,table(default.val, pred.log))
```
The error rate
```{r}
(err.log2 <- with(Default, mean(!default.val == pred.log)))
```

Any improvement?
```{r}
c(err.log,err.log2,err.null)

```

Again, we haven't done any train+test analysis. 

As before, we can group by students and see how we did in each class.

```{r}
Default %>%
    group_by(student)%>%
    summarize(err=mean(!default.val == pred.log),
              err.null=mean(default=="Yes"),
              err.ratio=err/err.null)


```

## The Separation Line for this model
The separation line occurs at at prob=0.5, equivalently odds=1, equivalently, log(odds)=0.

$$logOdds=b0+b1*balance+b2*income=0$$
Or,
$$income = -b0/b2 - b1/b2*balance$$


```{r}
summary(mod.log2)
coeffs <- coefficients(mod.log2)

ggplot(Default,aes(balance,income, color=default))+
    geom_point(size=1,alpha=0.5)+
    scale_color_brewer(palette="Set1")+
    ggtitle("Default by balance and income")+
    geom_abline(intercept=-coeffs[1]/coeffs[3],
                slope=-coeffs[2]/coeffs[3],size=2)

```


## Three predictors?
```{r}
mod.log3 <- glm(default~balance+income+student,
                family=binomial,
                data=Default)
summary(mod.log3)
```


```{r}
Default$prob.log <-predict(mod.log3,data=Default,type="response")
```

```{r}
threshVals<-seq(0,0.5,length.out = 100)
err.preds<-map_dbl(threshVals,calcErr)
plot(threshVals,err.preds)

min(err.preds)
id.min<-which.min(err.preds)
threshVals[id.min]
```


Confusion matrix
```{r}
Default<-Default %>% 
  mutate(pred.log=ifelse(prob.log<threshVals[id.min],0,1))
with(Default,table(default.val, pred.log))
```
error rate
```{r}
(err.log3 <- with(Default, mean(!default.val == pred.log)))
```

A modest improvement
```{r}
c(err.log,err.log2,err.log3,err.null)

```

## Assignment
Use train+test data with these three  logistic regression models

* One Predictor: balance
* Two Predictors: balance and income
* Three Predictors: balance, income, and student

In each case, create a logistic regression model (as above). Determine the best threshold value by computing training error rates over a sequence of threshold values. For each value, use a train+test combination to determine your estimate of the error rate. Report the best threshold value and the best error rate. 

#Cost Functions and optimization


Error rates are ok, but often there are separate "costs" associated
with each of the four possible classication states.
*Reward for True Negative: Correctly classify a non-default
*Reward for True Postive: Correctly classify a default
*Penalty for False Postive: a non-defaulter classified as a defaulter)
*Penalty for False Negative: a defaulter classified as a non-defaulter)


Build a cost function
Define costs for each of the four cases
These are subjective, context dependent


True Negative: reward or break even
```{r}
cost.tn <- 0 

```
True Postive: reward
```{r}
cost.tp <- 0

```
False Postive: Some cost
```{r}
cost.fp <-5

```
False Negative: Bigger cost
```{r}
cost.fn <- 25000

```

Assign costs.
What's the best way to do this?
One idea: Build a cost data frame

```{r}
cost.df <- data.frame(default.val=rep(c(1,0),each=2),
                      pred=rep(c(1,0),2),
                      cost=c(cost.tp,cost.fn,cost.fp,cost.tn))
cost.df
```
Now we can "join" it to Default using the default, and default.pred as "keys"

The join
```{r}
Default.cost <- inner_join(Default,
                           cost.df,
                           by=c("default.val"="default.val",
                                "pred.log"="pred"))

```

Did this work??
```{r}
with(Default.cost,table(default.val,pred.log))
with(Default.cost,table(default,cost))
with(Default.cost,table(cost))
```

Now we can change the threshold and see how if affects the cost function
```{r}
thresh.pred <- thresh.best
Default.cost <- Default%>%
    mutate(pred.log=
               ifelse(prob.log < thresh.pred,
                             0,1))%>%
  inner_join(cost.df,
             by=c("default.val"="default.val",
                  "pred.log"="pred"))
with(Default.cost,table(default.val,pred.log))
(theCost<-with(Default.cost, sum(cost)))
```


In other words, with the threshold of p=0.5, the total cost is `r theCost`. Is this good or bad? Compared to what?

Make the cost calculation a function.
```{r}
calcCost <- function(thresh.pred) {
  Default.cost <- Default%>%
      mutate(pred.log=
                 ifelse(prob.log < thresh.pred,
                               0,1))%>%
    inner_join(cost.df,
               by=c("default.val"="default.val",
                    "pred.log"="pred"))
  with(Default.cost, sum(cost))
}

calcCost(0.1)
calcCost(0.2)
calcCost(0.8)

```

Now create a list of these values and plot them. This will show
where the optimal threshold value is located

```{r}
N <- 30
probs <- seq(0.0000001,.99,len=N)
costVals<-map_dbl(probs,calcCost)
costVals
```

The plot of costs as function of threshold values.

```{r}
data.frame(prob=probs,cost=costVals) %>% 
  ggplot()+
  geom_point(aes(x=probs,y=costVals))+
  geom_line(aes(x=probs,y=costVals))+
  ggtitle("Costs curve for threshhold values")

```
It looks as if the optimal threshold for this cost function is somewhere around p=0.2 or so.

*Note:* 

  * Of course, we really want to train+test or use bootstrapping this on the data to more effectively determine the optimal threshhold.
  * Different cost functions lead to different optimal thresholds.




# Assignment  

Your task is to create a model to predict "success" of a candidates based on performance metrics You have two data sets:

* trainPerformance.csv
* testPerformance.csv

In each data set, there are 11 fields. The first 10 are performance metrics, the last one is a boolean (True/False) field indicating if the candidate was a "star" the after being hired. 

The two sets have identical structures. The plan is to build your model on the training data, and evaluate it (test it) on the the test data. Your work is scored by a cost matrix
## Cost Matrix
* **Predict** **Actual** **Cost**
* FALSE    FALSE     0    (no risk/no reward)
* NO       TRUE      20   (oops, undervalued a candidate)
* TRUE     FALSE     50   (bigger oops, overvalued a candidate)
* TRUE     TRUE     -10  (a reward! you picked a winner)


Build a logistic regression model with up to five predictors number (no interactions)  to predict next year's performance outcome status. Find an optimal a probability threshhold that will maximize the total reward.

Also, if you want to, build a KNN model to for the same task. With KNN, you will have to optimize on both the threshold and the the value of k (nearest neighbors).



