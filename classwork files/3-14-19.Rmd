---
title: "3-14-19"
author: "Nathan Kurtz-Enko"
date: "3/13/2019"
output: pdf_document
---
#to start out class, think of covariance

```{r}
#this is all wrong or something is wrong, GO OVER THIS
n <- 10
x <- runif(n, -1, 1)
y <- runif(n, -3, 3)

X <- matrix(c(x,y), ncol=2)

cov(X)
t(x)

mean_x <- mean(x)
mean_y <- mean(y)

diff_x <- x-mean_x
diff_y <- x-mean_y

X1 <- matrix(c(diff_x,diff_y), ncol = 2)

my_cov <- 1/(n-1)*t(X1) %*% X1
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,cache = TRUE)
```

###Load tidyverse, of course.
```{r}
library(tidyverse)

```


#Cross Validation Introduction


### Build some simple data to practice on. Use the regression model.
```{r}
N<-100
sig<-5/sqrt(5)
b0  <- 1
b1  <- 2

```



Here it is...

```{r}
x <- rnorm(N,0,1) 
y <- b0+b1*x+rnorm(N,0,sig)
data.df<-data.frame(x,y)

```



what are we looking at?

```{r}
data.df %>%
  ggplot()+
  geom_point(aes(x,y))


```



###A linear model and preditions
```{r}
mod1  <- lm(y~x,data=data.df)
data.df$pred1 <- predict(mod1)  

```



Training MSE
```{r}
(mse.train<-with(data.df,mean((y-pred1)^2)))


```



How about some test data?
```{r}
x <- rnorm(N,0,1) 
y <- b0+b1*x+rnorm(N,0,sig)
test.df<-data.frame(x,y)

```



Test prediction and MSE
```{r}
test.df$pred1 <- predict(mod1,newdata=test.df)  

(mse.test<-with(test.df,mean((y-pred1)^2)))


```



#k-fold Cross Validation


```{r}
numFolds<-5
N<-nrow(data.df)
folds<-sample(1:numFolds,N,rep=T)

```



Here's how to build the test/train combo
Pull out the first fold
```{r}
train.df  <- data.df[folds != 1,]
test.df   <- data.df[folds == 1,]

```



Now the model and test mse

```{r}
mod.cv  <- lm(y~x,data=train.df)
test.df$pred1 <- predict(mod1,newdata=test.df)  
(mse.cv <- with(test.df,mean((y-pred1)^2)))
c(mse.train,mse.test,mse.cv)


```



##Repeat for all the folds. Keep track of the results 
and take the mean

```{r}
mseKFold<-numeric(numFolds)
for(fold in 1:numFolds){
  train.df  <- data.df[folds != fold,]
  test.df   <- data.df[folds == fold,]
  mod.cv  <- lm(y~x,data=train.df)
  test.df$pred1 <- predict(mod.cv,newdata=test.df)  
  mseKFold[fold] <- with(test.df,mean((y-pred1)^2))
}
(mse.kfold <- mean(mseKFold))

```



#Leave one out CV (LOOCV)


Now we get N cv mse values

```{r}
mseLOOCV<-numeric(N)
for(n in 1:N){
  train.df  <- data.df[-n,]
  test.df   <- data.df[n,]
  mod.loocv  <- lm(y~x,data=train.df)
  test.df$pred1 <- predict(mod.loocv,newdata=test.df)  
  mseLOOCV[n] <- with(test.df,mean((y-pred1)^2))
}
(mse.loocv <- mean(mseLOOCV))


```



#Bootstrap
Last, but not least, bootstrap

```{r}
B<-100
mseBoot<-numeric(B)
for(b in 1:B){
  boots<-sample(1:N,N,rep=T)
  train.df  <- data.df[boots,]
  test.df   <- data.df[-boots,]
  mod.boot  <- lm(y~x,data=train.df)
  test.df$pred1 <- predict(mod.boot,newdata=test.df)  
  mseBoot[b] <- with(test.df,mean((y-pred1)^2))
}
mse.boot <- mean(mseBoot)


```



#Train/test with synthetic data.
Build train/test combos
Note: this only works for synthetic data where we can build as much
data as we need.

###Construct a data building function
```{r}
buildData<-function(N){
  x <- rnorm(N,0,1) 
  y <- b0+b1*x+rnorm(N,0,sig)
  data.frame(x,y)
}


```


###Here we go...build a large number of train/test combos
```{r}
M<-10000
mseTT<-numeric(M)
for(m in 1:M){
  train.df<-buildData(N)
  test.df<-buildData(N)
  mod<-lm(y~x,data=data.df)
  preds<-predict(mod,newdata=test.df)
  mseTT[m]<-with(test.df,mean((y-preds)^2))
}
mse.tt <- mean(mseTT)
mse.tt

```


###Compare results...your mileage will vary
```{r}
c(mse.cv,mse.loocv, mse.boot, mse.tt)

```



#Using CV to identify parameters

An important task for CV is to help determine optimal
parameter selection. In this case, we don't care so much what the
mse estimate is, we care that is identifying the optimal value. 


##KNN and CV...best k
```{r}
library(FNN) ## for knn.reg

```



Build some nonlinear data
```{r}
f<-function(x){
  2+sin(3*x)
}


```



What are we looking at here?

```{r}
x<-seq(-2*pi,2*pi,by=.01)
y<-f(x)
plot(x,y)

```



###Build the data
```{r}
N<-200
sig<-2/sqrt(5)
buildData<-function(N){
  x <- rnorm(N,0,3) 
  y <- f(x)+rnorm(N,0,sig)
  data.frame(x,y)
}

data.df<-buildData(N)

```



Plot it....
```{r}
data.df %>%
  ggplot()+
  geom_point(aes(x,y))

```



###Structure the data to use knn.reg
```{r}
train.dat<-data.df["x"]
resp<-with(data.df,y)

```



##The KNN model
```{r}
kval <- 11
mod.knn<-knn.reg(train.dat,train.dat,resp,kval)

```



###Predictions and results
```{r}
pred<-mod.knn$pred
with(data.df,mean((y-pred)^2))

```



#Lab Assignment
Model this situation with knn.reg.
Use each of KFold, LOOCV, and bootstrap 
to find the best value of k. Do they agree?
Plan: 
* build your data
* create functions mseKFold, mseLOOCV, mseBOOT
that will compute the mse estimate on a training 
dataset using a fixed value of k.

For each of these, compute the mse for a sequence of k values.
Identify your choice of the optimal k value.

Since this is synthetic data, repeat this process for a
(reasonably) large number of train/test combinations. Use the 
results of these combos to estimate the optimal k value. Do your
results via CV and bootstrapping agree with the train/test result? 


```{r}
#build data
N<-200
sig<-2/sqrt(5)
buildData<-function(N){
  x <- rnorm(N,0,3) 
  y <- f(x)+rnorm(N,0,sig)
  data.frame(x,y)
}

```

#kfold

```{r}

#define a functions to evaluate cross validation for particular k nearest neighbor
mse_Kfold <- function(k){
  numFolds<-10
  folds<-sample(1:numFolds,N,rep=T)
  mseKFold<-numeric(numFolds)
  for(fold in 1:numFolds){
    train.df  <- data.df[folds != fold,]
    test.df   <- data.df[folds == fold,]
    train.dat<-train.df["x"]
    resp<-with(train.df,y)
    test.dat<-test.df["x"]
    mod.knn  <- knn.reg(train.dat,test.dat,resp,k)  
    mseKFold[fold] <- with(test.df,mean((y-(mod.knn$pred))^2))
    }
  (mse.kfold <- mean(mseKFold))
}
```

```{r}
#define kvals and evaluate over them and find the best one

kval <- 2:20
mses<- tibble(k = kval, mse = kval)
for(k in kval){
  mses[k-1,2] = mse_Kfold(k)
}

min_mse <- min(mses$mse)

best_k <- (filter(mses, mse == min_mse))$k

best_k
```


```{r}
best_mse <- mse_Kfold(best_k)
best_mse
```

```{r}
#see how this looks
ggplot(mses)+
  geom_point(aes(x = k, y = mse))+
  geom_vline(xintercept = best_k)
```

#Leave one out cv

```{r}
#build function
mse_LOOCV <- function(k){
  N<-nrow(data.df)
  mseLOOCV<-numeric(N)
  for(n in 1:N){
    train.df  <- data.df[-n,]
    test.df   <- data.df[n,]
    train.dat<-train.df["x"]
    resp<-with(train.df,y)
    test.dat<-test.df["x"]
    mod.knn  <- knn.reg(train.dat,test.dat,resp,k)  
    mseLOOCV[n] <- with(test.df,mean((y-(mod.knn$pred))^2))
  }
  (mse.loocv <- mean(mseLOOCV))
}
```

```{r}
#define kvals and evaluate over them using function and find best kval
kval <- 2:20
mses<- tibble(k = kval, mse = kval)
for(k in kval){
  mses[k-1,2] = mse_LOOCV(k)
}

min_mse <- min(mses$mse)

best_k <- (filter(mses, mse == min_mse))$k

best_k
```

```{r}
best_mse <- mse_LOOCV(best_k)
best_mse
```

```{r}
#see how this looks
ggplot(mses)+
  geom_point(aes(x = k, y = mse))+
  geom_vline(xintercept = best_k)
```


#mse bootstrapping

```{r}
#define our function
mse_Boot <- function(k){
  N <- nrow(data.df)
  B<-100
  mseBoot<-numeric(B)
  for(b in 1:B){
    boots<-sample(1:N,N,rep=T)
    train.df  <- data.df[boots,]
    test.df   <- data.df[-boots,]
    train.dat<-train.df["x"]
    resp<-with(train.df,y)
    test.dat<-test.df["x"]
    mod.knn  <- knn.reg(train.dat,test.dat,resp,k)  
    mseBoot[b] <- with(test.df,mean((y-(mod.knn$pred))^2))
  }
  (mse.boot <- mean(mseBoot))
}
```

```{r}
#define kvals and evaluate over them using function and find best kval
kval <- 2:20
mses<- tibble(k = kval, mse = kval)
for(k in kval){
  mses[k-1,2] = mse_Boot(k)
}

min_mse <- min(mses$mse)

best_k <- (filter(mses, mse == min_mse))$k

best_k
```

```{r}
best_mse <- mse_Boot(best_k)
best_mse
```

```{r}
#see how this looks
ggplot(mses)+
  geom_point(aes(x = k, y = mse))+
  geom_vline(xintercept = best_k)
```



#Homework Assignment: Wine Quality Prediction

Go to: https://archive.ics.uci.edu/ml/datasets/Wine+Quality
Use the white wine data set. Build a model to predict quality as
a function of the predictors. Compare linear regression with KNN 
(using knn.reg)
For linear regression, use CV and/or bootstrap to determine the best (or at least a good)
set of predictors.
For KNN, determine the best choice of k.

Note: before starting the modeling, scale the data!


Pack the data into a data frame.

```{r}
data.df<-data.frame(x=c(dat1,dat2),
                    y=c(ydat1,ydat2),
                    class=factor(c(rep(1,N1),rep(2,N2))))
```

And, of course, take a look at what you have.
```{r}
data.df %>%
  ggplot()+
  geom_point(aes(x,y,color=class))+
   _color_manual(values=c("red","blue"))

```


From here, use LDA as we did in class to build a model. Add a grid to highlight the decision boundary. You can approximatly infer the linear equation from this graph.

You goal is to show that your inferred decision boundary corresponds to what is given by Formula 4.19. More precisely,
when
$$ \delta_1(x)=\delta_2(x)$$

To do so, you will need to create the function described in Formula 4.19. This means you will need to use R's matrix multiplication (i.e. %*% )

Create both $\delta_1$ and $\delta_2$ and, as before, use these as linear classifiers. Do you reconstruct (using a grid etx) the same decision regions as those produced by LDA?