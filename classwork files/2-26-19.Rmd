---
title: "2-26-19"
author: "Nathan Kurtz-Enko"
date: "2/26/2019"
output: pdf_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,cache = TRUE)
```
```{r}
library(tidyverse)
library(broom)

```


#Introduction:  Linear Regression
Let's take a quick tour of linear regression.

Underlying assumption: linear model
$y=f(x_1,x_2,\dots,x_n)+\epsilon$ where $f$ is linear in the inputs
$x_1,\dots, x_n$ and $\epsilon \sim N(0,\sigma^2)$. Equivalently,
we say $E[y|x]=f(x_1,x_2,\dots,x_n)$.
##Toy Example
```{r First Ch}
N <- 50

b0 <- 1
b1 <- 3

sigma <- 2

x <- rnorm(N,0,1)
y <- b0+b1*x+rnorm(N,0,sigma)
data.df <- data.frame(x,y)

ggplot(data.df)+
    geom_point(aes(x,y))

mod <- lm(y~x,data=data.df)
summary(mod)

```


Just the coefficients: intercept and slope
These are computed directly from the data.
In effect, the data with N points, has been reduced to 2 numbers.
```{r}
mod$coef

```


Visual analysis of the linear model
```{r}
plot(mod)

```


predict on the training data
```{r}
preds <- predict(mod)
data.df$pred <- preds
head(data.df)

```


MSE
MSE is a good estimator (slightly biased here) of variance
```{r}
mse <- with(data.df,mean((y-pred)^2))
mse
sigma^2


```


Our linear model
Plot predicted values
```{r}
gg1 <- ggplot(data.df)+
    geom_point(aes(x,y),color="blue")+
    geom_point(aes(x,preds),color="red")+
    ggtitle("Linear Fit")
gg1

```


Or, since this is a linear model, use the coefficients
```{r}
gg1 <- ggplot(data.df)+
    geom_point(aes(x,y),color="blue")+
    geom_abline(aes(intercept=mod$coef[1],slope=mod$coef[2]),color="red")+
    ggtitle("Linear Fit")
gg1

```



The residuals are visualized as the vertical difference between the
predictions and the actual values.
```{r}
ggplot(data.df)+
    geom_point(aes(x,y),color="blue")+
    geom_abline(aes(intercept=mod$coef[1],slope=mod$coef[2]),color="red")+
    geom_segment(aes(x=x,xend=x,y=y,yend=preds),color="black")+
    ggtitle("Linear Fit with Residuals")

```


Fact: the coefficients of the linear model minimize the mse. In
other words, if $f(x)=a_0+a_1x$ is any linear predictions, we can
define mse(a_0,a_1)$ as the average squared loss over the
data. This functions is minimized when $(a_0,a_1)$ equal the values
given by lm.

##Assignment.
For our scenario, define the function
$$mse(a_0,a_1)=\frac{1}{N}\sum_{i=1}^N (y-(a_0+a_1x))^2$$
Evaluate mse over a grid of values surrounding the values of
b0=`r  b0` and b1=`r b1` above. Show mse is mimimal at the coefficient
values given by lm (slope = `r mod$coef[1]` and intercept=`r mod$coef[2]`).

```{r}
b0 <- 1
b1 <- 3

N <- 50
sigma <- 2

b0_grid <- seq(0,2,.1)
b1_grid <- seq(2,4,.1)

x <- rnorm(N,0,1)
y <- b0+b1*x+rnorm(N,0,sigma)
data_tb <- tibble(x = x, y = y)

mod1 <- lm(y~x, data = data_tb)
coefficients1 <- mod1$coefficients
min_intercept <- coefficients1[1]
min_slope <- coefficients1[2]

mse_tb <- tibble(intercept = b0_grid, slope = b1_grid, mse = 1:21)

mse_function <- function(df){
  x <- 
}

buildData<-function(N){
  rndMean <- sample(1:numMeans,N,rep=T)
  vals0<- t(apply(mu0[rndMean,],1,function(mu) mvrnorm(1,mu,diag(2)*sd)))
  rndMean <- sample(1:numMeans,N,rep=T)
  vals1 <- t(apply(mu1[rndMean,],1,function(mu) mvrnorm(1,mu,diag(2)*sd)))
  vals <- rbind(vals1,vals0)
  data.frame(row=1:(2*N),x=vals[,1],y=vals[,2],class=rep(c("A","B"),each=N))
}

```



##Prediction Intervals
A prediction interval describes the region in which you can expect
to find the predicted mean at each input value. There are formulas
for the prediction values.

Here is how you produce 95% prediction intervals  in R.
```{r}
xvals <- seq(-5,5,length=100)
pred1 <-
    predict(mod,interval="confidence",newdata=data.frame(x=xvals),level=0.95)
pred2.df <- data.frame(pred1)

```


and a plot....
```{r}
gg.pred <- ggplot(data.df)+
    geom_point(aes(x,y),color="blue")+
    geom_line(data=pred2.df,aes(x=xvals,y=fit),color="green")+
    geom_line(data=pred2.df,aes(x=xvals,y=lwr),color="green")+
    geom_line(data=pred2.df,aes(x=xvals,y=upr),color="green")+
        ggtitle("95% Interval for Means")
gg.pred


```


R will also give a predicted response interval. The predicted
response takes into account both the variability of the mean and
variability of the response be a random variable about the
mean. There are analytic expressions for this value as well.

```{r}
pred2 <- predict(mod,interval="prediction",newdata=data.frame(x=xvals),level=0.95)
pred2.df <- data.frame(pred2)

```


and the plot. Notice this captures almost all of the points.
```{r}
gg.pred2 <- ggplot(data.df)+
    geom_point(aes(x,y),color="blue")+
    geom_line(data=pred2.df,aes(x=xvals,y=fit),color="green")+
    geom_line(data=pred2.df,aes(x=xvals,y=lwr),color="green")+
    geom_line(data=pred2.df,aes(x=xvals,y=upr),color="green")+
    ggtitle("95% Interval for Responses")
gg.pred2


```



#Bootstrapping

We can repeat most of what we saw above by digging the variability
information out of the data.

A bootstrap sample is a sample of the same size as the original
data set obtained by sampling the original data n times with
replacement. Thanks Brad Efron!
[Check out](https://en.wikipedia.org/wiki/Bootstrapping_(statistics))
for more information on bootstrapping. We will be looking at this
in more detail later.

Here's how it goes..
```{r}
n <- nrow(data.df)
bootSample <- sample(1:n,n,rep=T)
boot.df <- data.df[bootSample,]

```


A bootstrapped model
```{r}
mod.boot <-lm(y~x,
              data=boot.df)

```


These two are close.
```{r}
summary(mod.boot)
summary(mod)

```



Bootstrapping is a way to simulate the generation of sample data,
without having to go get more data

Let's use bootstrapping to estimate the variability of the
estimated coefficients
Plan: Repeat the bootstrapping M times
Keep track of the estimated coefficients each time
```{r}
M <- 500
n <- nrow(data.df)
coefs.boot <- matrix(nrow=M,ncol=2)
for(m in 1:M){
    bootSamp <- sample(1:n,n,rep=T)
    boot.df <- data.df[bootSamp,]
    mod.boot <-lm(y~x,
                  data=boot.df)
    coef.boot <- coef(mod.boot)
    coefs.boot[m,] <- coef.boot
}

```


A histogram of the estimated slopes
```{r}
hist(coefs.boot[,2],breaks=50)

```


The Means and standard deviations of the coefficients
```{r}
apply(coefs.boot,2,mean)
apply(coefs.boot,2,sd)


```


Compare this the original linear model. Pay special attention to the
estimates and their SEs.
```{r}
summary(mod)


```


We can also use bootstrapping to estimate the prediction interval
for the means (as we did above).
```{r}
coefs.df <- data.frame(coefs.boot,boot=rep(1:M))
names(coefs.df) <- c("intercept","slope","boot")

ggplot(data.df)+
    geom_point(aes(x,y),color="blue")+
    geom_abline(data=coefs.df,
                aes(intercept=intercept,slope=slope,group=boot),
                color="red",
                alpha=0.05)+
    ggtitle("Estimated Means Bootstrapping")
gg.pred


```


If we want the 95% coverage intervals corresponding the prediction
intervals, we can get them via bootstrapping
```{r}
n0 <- 100
xvals <- seq(-5,5,length=n0)
pred.df <- data.frame(x=xvals)
preds.boot <- matrix(nrow=M,ncol=n0)
for(m in 1:M){
    bootSamp <- sample(1:n,n,rep=T)
    boot.df <- data.df[bootSamp,]
    ##
    mod.boot <-lm(y~x,
                  data=boot.df)
    preds.boot[m,] <- predict(mod.boot,newdata=pred.df)
}

```



A couple helper functions needed to get the upper and lower
```{r}
getLim <- function(ls,lim) {
    quantile(ls,probs=lim,na.rm=T)
}
getLwr <- function(ls) {
    quantile(ls,probs=0.05,na.rm=T)
}

```


Calculate the mean of the prediction and the upper and lower
limits for 95% coverage
```{r}
pred.df$pred<- apply(preds.boot,2,mean)
pred.df$upr <- apply(preds.boot,2,function (ls) getLim(ls,0.95))
pred.df$lwr <- apply(preds.boot,2,function (ls) getLim(ls,0.05))


```


and a plot...
```{r}
gg.pred.bs <- ggplot(data.df)+
    geom_point(aes(x,y),color="blue")+
    geom_line(data=pred.df,aes(x=x,y=pred),color="red")+
    geom_line(data=pred.df,aes(x=x,y=upr),color="red")+
    geom_line(data=pred.df,aes(x=x,y=lwr),color="red")+
    ggtitle("Estimated Means Bootstrapping")
gg.pred.bs

```


compare with...
```{r}
gg.pred
```


very similar

#Mean Squared Error via bootstrapping
MSE...this is what we are most interested in.
boot strap MSE!

```{r}
M <- 1000
mses.boot <- array(dim=M)
for(m in 1:M){
    bootSamp <- sample(1:n,n,rep=T)
    boot.df <- data.df[bootSamp,]
    ##
    mod.boot <-lm(y~x,
                  data=boot.df)
    preds <-  predict(mod.boot)
    mses.boot[m] <-with(boot.df,mean((y-preds)^2))
}

```


Let's see what we have.
```{r}
hist(mses.boot,breaks=100)
```


Should compare well with MSE seen earlier. It's nice to get an idea
of variability as well.
```{r}
mean(mses.boot)
var(mses.boot)


```



#Prediction without linear models
Let's look at an example of prediction on the same data set that
doesn't use a linear model. We will use the loess (local
regression). In many ways, it looks a lot like using a linear model.
```{r}
mod.loess <- loess(y~x,span=0.8,data=data.df)


```


Make predictions...
```{r}
preds <- predict(mod.loess)
data.df$pred.loess <- preds

```


MSE is always computed the same way.
```{r}
mse <- with(data.df,mean((y-pred)^2))
mse.loess <- with(data.df,mean((y-pred.loess)^2))
mse.loess
mse



```



Plotvalues from the loess prediction
```{r}
ggplot(data.df)+
    geom_point(aes(x,y),color="blue")+
    geom_line(aes(x,pred.loess),color="red")+
    ggtitle("Loess Prediction")




```



There are no analytic formualas for the prediction interval for the
loess model. However, we can get them via bootstrapping just as before

Repeat the bootstrapping M times
Keep track of the estimated coefficients each time
```{r}
M <- 500
n <- nrow(data.df)
pred.df <- data.frame(x=xvals)
preds.boot <- matrix(nrow=M,ncol=n0)
for(m in 1:M){
    bootSamp <- sample(1:n,n,rep=T)
    boot.df <- data.df[bootSamp,]
    ##
    mod.boot <-loess(y~x,span=0.8,
                  data=boot.df)
    preds.boot[m,] <- predict(mod.boot,newdata=pred.df)
}

```


Upper and lower limits, along with the predictions
```{r}
pred.df$pred <- apply(preds.boot,2,function(ls) mean(ls,na.rm=T))
pred.df$upr <- apply(preds.boot,2,function (ls) getLim(ls,0.95))
pred.df$lwr <- apply(preds.boot,2,function (ls) getLim(ls,0.05))

```


and the graph...
```{r}
gg.pred.lo <- ggplot(data.df)+
    geom_point(aes(x,y),color="blue")+
    geom_line(data=pred.df,aes(x,pred),color="red")+
    geom_line(data=pred.df,aes(x,upr),color="red")+
    geom_line(data=pred.df,aes(x,lwr),color="red")+
    ggtitle("Loess Prediction Intervals")

```


compare prediction intervals
```{r}
gg.pred.lo+scale_y_continuous(limits=c(-10,10))
gg.pred.bs+scale_y_continuous(limits=c(-10,10))

```


We can also do  MSE estimation via bootstrapping
```{r}
M <- 1000
mses.boot.lo <- array(dim=M)
for(m in 1:M){
    bootSamp <- sample(1:n,n,rep=T)
    boot.df <- data.df[bootSamp,]
    ##
    mod.boot <-loess(y~x,span=0.8,
                     data=boot.df)
    preds <-  predict(mod.boot,newdata=boot.df)
    mses.boot.lo[m] <-with(boot.df,mean((y-preds)^2))
}
```


Here's what we get.
```{r}
hist(mses.boot.lo)
mean(mses.boot.lo)
var(mses.boot.lo)

```



##Other data-centric approaches to MSE estimations
Here is an artificial scenario. We recreate the  data M times, each
time build a train and test combo.
```{r}
M <- 1000
mse.vals <- matrix(nrow=M,ncol=2)
for(m in 1:M){
    x <- rnorm(N,0,1)
    y <- b0+b1*x+rnorm(N,0,sigma)
    data.df <- data.frame(x,y)
    x <- rnorm(N,0,1)
    y <- b0+b1*x+rnorm(N,0,sigma)
    test.df <- data.frame(x,y)
    mod.lm <- lm(y~x)
    mod.lo <- loess(y~x,span=0.8)
    pred.lm <- predict(mod.lm,newdata=test.df)
    pred.lo <- predict(mod.lo,newdata=test.df)
    mse.lm <- with(test.df,mean((y-pred.lm)^2))
    mse.lo <- with(test.df,mean((y-pred.lo)^2))
    mse.vals[m,] <- c(mse.lm,mse.lo)
}


```


The means of the mse values
```{r}
apply(mse.vals,2,mean)

```


fix up the data so it can be plotted.
```{r}
colnames(mse.vals) <- c("LM","LOESS")
df <- data.frame(mse.vals)%>%
    gather(type,val,1:2)


```


Now plot
```{r}
ggplot(df)+
    geom_density(aes(val,..density..,fill=type),alpha=0.5)



```


#Assignment: MSE comparison via bootstrapping
In real life, we can't create M different train+test combos. But we
can bootstrap. Using a single training set (e.g. a single data.df),
bootstrap M times to estimate the MSE for both the linear model and
loess. Is there any evidence that one algorithm is better than the other?



Linear Model or Loess: Which one is better? How would we decide?
Consider the the data set kidneyCASI.txt

 *  *age* is age of volunteer
 *  *tot* is composite kidney health score.


```{r}
dataDir<-"~/ADM/class/"
fileName<-"kidneyCASI.csv"

kidney.df <- read.csv(file.path(dataDir,fileName))
ggplot(kidney.df)+
    geom_point(aes(age,tot))
```

```{r}
#begin bootstraping
m <- 500
n <- length(kidney.df$age)

mse_bootstrapping <- tibble(mth_boot = 1:m, mse = 1:m)

for(m in 1:M){
    bootSamp <- sample(1:n,n,rep=T)
    boot.df <- kidney.df[bootSamp,]
    mod.boot <-loess(tot~age,span=0.8,
                     data=boot.df)
    preds <-  predict(mod.boot,newdata=boot.df)
    mse_bootstrapping[m, 2] = mean((tot-preds$tot)^2)
}
```


Note: Kidney's start to deteriorate as people get older!

# Assignment

Assess the utility of both lm and loess (span=0.3 here) in terms of
prediction. In particular, summarize the difference in predictions
and the standard error of prediction for each algorithms. You can
use bootstrapping for both algorithms, though for the lm, there are
analytic formulas avaialbe. Are there
any marked differences between these two algorithms?

Use bootstrapping to estimate the MSE of each algorithm. Is there
any reason to favor one method over the other based on MSE?

Can you improve the performance (i.e., decrease MSE) by adding more
"features"? In this case, this means adding higher powers of the
input "age".
One simply way to do this is simply to mutate the data frame by
adding in the powers

```{r}
kidney.df <- kidney.df%>%
    mutate(age2=age*age,
           age3=age*age*age)

```


