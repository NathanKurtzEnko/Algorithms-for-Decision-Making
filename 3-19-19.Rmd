---
title: "3-19-19"
author: "Nathan Kurtz-Enko"
date: "3/19/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##0 Introduction to Penalized Regression and Bias-Variance


#######################################################
## We will be looking at Ridge Regression vs Linear Regression
## Bias/Variance Effect on coefficient estimation
##
## **Note:** This example uses lm.ridge. ISLR uses glmet. We will use
## both.
#######################################################


## Load the libraries
```{r setup, include = FALSE}
library(MASS)
library(tidyverse)
```

##Set up some paraments
## p is the number of predictors
##N is the sample size

```{r}
p <- 2
N <- 4
```

#######################################################

## Build the model with regression coefficients b1 and b2 (no
## intercept) and with error term with sd

```{r}
sd <- 2
b1 <- 2
b2 <- 2
```

#######################################################

### Generate a number of data seta build  Ordinary Least Squares (OLS) models. Keep track of
## the estimated coefficients. These should be unbiased estimators of
## the true value

#########################################

```{r}
K <- 200
coef.est <- matrix(0,nrow=K,ncol=2)
for(k in 1:K){
  x1 <- rnorm(N,0,1)
  x2 <- rnorm(N,0,1)
  x1 <- x1-mean(x1)
  x2 <- x2-mean(x2)
  ##no constant term
  y <- b1*x1+b2*x2+rnorm(N,0,sd)
    data.df <- data.frame(x1,x2,y)
  ##no constant term    
  mod.lm <- lm(y~x1+x2+0,data=data.df)
  cc.lm <- coefficients(mod.lm)
  coef.est[k,] <- cc.lm
}


## Pack into a data frame
coef_lm.df <- data.frame(b1.est=coef.est[,1],
                      b2.est=coef.est[,2],
                      type="lm")

## Here are the coefficient estimates
## If all goes well, we should see an unbiased estimate.
ggplot(coef_lm.df,aes(b1.est,b2.est))+
    geom_point(color="blue")+
    ##True values of b1 and b1
    geom_point(aes(b1,b2),size=3,color="black")+
  scale_x_continuous("b1 est")+
  scale_y_continuous("b2 est")+    
  ggtitle("Bias and Variance: LM")
```



#######################################################

## Build a both a ridge regression (with lambda.val) and linear model
## Keep track of the coefficient estimates for each of K iterations
## Note: the data has  mean =0

#######################################################

## Arbitrary choice of lambda

```{r}
lambda.val <- 5
K <- 200
coef.est <- matrix(0,nrow=K,ncol=2)
for(k in 1:K){
    x1 <- rnorm(N,0,1)
    x2 <- rnorm(N,0,1)
    x1 <- x1-mean(x1)
    x2 <- x2-mean(x2)
    ##no constant term
    y <- b1*x1+b2*x2+rnorm(N,0,sd)
    data.df <- data.frame(x1,x2,y)
    mod.ridge<- lm.ridge(y~x1+x2+0,data=data.df,lambda=lambda.val)
    cc.ridge <- coefficients(mod.ridge)
    coef.est[k,] <- cc.ridge
}


## Pack these together
coef_ridge.df <- data.frame(b1.est=coef.est[,1],
                      b2.est=coef.est[,2],
                      type="ridge")
### and a plot...we should see some bias now.
ggplot(coef_ridge.df,aes(b1.est,b2.est))+
  geom_point(color="red")+
  geom_point(aes(b1,b2),size=3,color="black")+
  ggtitle("Bias and Variance: Ridge")


## Combine the two coefficient sewts
coef.df<-bind_rows(coef_lm.df,coef_ridge.df)
## look at both together
ggplot(coef.df)+
  geom_point(aes(b1.est,b2.est,color=type))+
  scale_color_manual(values=c("blue","red"))+
  geom_point(aes(b1,b2),size=3,color="black")+
  ggtitle("Bias and Variance: LM and Ridge")

```

#######################################

## What abot the error?

```{r}
coef.df %>%
  group_by(type)%>%
  summarize(err1 = mean((b1-b1.est)^2),
            err2 = mean((b2-b2.est)^2))


## Repeat the computation with different lambda value.
```

## In this example, we will build a synthetic data set with two types of
## predictors.
## One subset of the predictors will be large (high influence), the other
## will be small. The goal is to see how Ridge and Lasso handle these predictors


## Load the packages
```{r setup, include=FALSE}
library(tidyverse)
library(MASS)
library(glmnet)
```

## Build a fairly wide data set.

```{r}
n<-150
p<-240

##large
nlarge <- 15
nsmall <- p-nlarge


##Select large coefficients, the rest are small
beta <- c(runif(nlarge,.55,1),runif(nsmall,0,0.3))
hist(beta,breaks=20)


##Pack up some data
X.train <- matrix(rnorm(n*p,0,1),ncol=p)
X.test <- matrix(rnorm(n*p,0,1),ncol=p)

sd <- 2
Y.train <- X.train %*% beta+rnorm(n,0,sd)
Y.test <- X.test %*% beta+rnorm(n,0,sd)
```

############################################
## First, a linear model with no y-intercept

```{r}
mod.lm <- lm(Y.train~0+X.train)
summary(mod.lm)


## Pull off the coefficients 
(coef.lm <- coef(mod.lm))

##in case there are some NAs
coef.lm[is.na(coef.lm)] <- 0

hist(coef(mod.lm),breaks=10)

pred.lm.train <- predict(mod.lm,newdata=list(X.train))
pred.lm.test <- predict(mod.lm,newdata=list(X.test))

mse.ols.train <- mean((Y.train-pred.lm.train)^2)
mse.ols.test <- mean((Y.test-pred.lm.test)^2)
##Compare the two..
c(mse.ols.train,mse.ols.test)
## Pretty radical difference!!
```

#######################################################

## Ridge regression.

## In glmnet, ridge regression uses alpha=0.
## To start, use a fixed lambda value

```{r}
lambda.val <- .1
mod.ridge <- glmnet(X.train,Y.train,alpha=0,lambda=lambda.val)


## take a look at the coefficients
##includes intercept=0

coef.ridge <- coef(mod.ridge)[-1,1]

## How do ridge coefficients compare with the OLS coefficients?
data.frame(coef.ridge=coef.ridge,
           coef.lm=coef.lm,
           coef=1:p) %>%
    ggplot()+
    geom_point(aes(coef.lm,coef.ridge),color="red")+
    geom_abline(slope=1,color="blue")+
    geom_label(aes(coef.lm,coef.ridge,label=coef),size=3)#+
    #coord_fixed()

## Compare the norms of each of these coefficient sets
sum(coef.lm^2)
sum(coef.ridge^2)

## Make a prediction on training data
pred.ridge.train <- predict(mod.ridge,newx=X.train)
mse.ridge.train <- mean((Y.train-pred.ridge.train)^2)
c(mse.ols.train,mse.ridge.train)


pred.ridge.test <- predict(mod.ridge,newx=X.test)
mse.ridge.test <- mean((Y.test-pred.ridge.test)^2)
c(mse.ols.test,mse.ridge.test)

```


############################################

## What is the best choice of lambda??
## glmnet will build models for a sequence of lambda values

```{r}
lambda.grid <- 10^seq(-3,1,length=100)

## build the ridge model, now there will ber a sequence of lambda
## values used
mod.ridge <-
    glmnet(X.train,Y.train,alpha=0,intercept=F,lambda=lambda.grid)


## What did we get??
summary(mod.ridge)
str(mod.ridge)


## a plot of the coefficients in the L1 Norm
plot(mod.ridge)

## glmmet has a built in cross-validation tool
cv.ridge <-
    cv.glmnet(X.train,Y.train,alpha=0,intercept=F,lambda=lambda.grid)
## here's how the mse looks a a function of the log(lambda)
plot(cv.ridge)

##extract the optimal lambda
(lambda.opt <- cv.ridge$lambda.min)


```

#################################

## Now build the model  on the optimal lambda

```{r}
mod.ridge <-
    glmnet(X.train,Y.train,alpha=0,intercept=F,lambda=lambda.opt)



## take a look at the coefficients
(coef.ridge <- coef(mod.ridge)[-1,1])
## Sum of squares...

sum(coef.ridge^2)

## Look at the scatter plot of ols vs ridge coefficents

data.frame(coef.ridge=coef.ridge,
           coef.lm=coef.lm,
           coef=1:p) %>%
    ggplot()+
    geom_point(aes(coef.lm,coef.ridge),color="red")+
    geom_abline(slope=1,color="blue")

## Predictions...
pred.ridge.train <- predict(mod.ridge,newx=X.train)
(mse.ridge.train <- mean((Y.train-pred.ridge.train)^2))

pred.ridge.test <- predict(mod.ridge,newx=X.test)
(mse.ridge.test <- mean((Y.test-pred.ridge.test)^2))

## compare the errors
## train
c(mse.ols.train,mse.ridge.train)
## test
c(mse.ols.test,mse.ridge.test)

## We decrease the error rate by a significant factor!
```

#######################################################

## Repeat all of the above with lasso
## Lasso alpha=1

```{r}
mod.lasso <- glmnet(X.train,Y.train,alpha=1,intercept=F,lambda=.1)

## Predict on training data
pred.lasso.train <- predict(mod.lasso,newx=X.train)

```



############################################

## build a grid of lambda values

```{r}
lambda.grid <- 10^seq(-3,1,length=100)
mod.lasso <- glmnet(X.train,Y.train,alpha=1,intercept=F,lambda=lambda.grid)
##

## The plot of the lasso coefficients really shows how they start to
## drop out
plot(mod.lasso)


## Use built-in cross-validation
cv.lasso <- cv.glmnet(X.train,Y.train,alpha=1,intercept=F,lambda=lambda.grid)
plot(cv.lasso)

lambda.opt <- cv.lasso$lambda.min
lambda.opt
```


#################################

## Build optimal lasso model

```{r}
mod.lasso <- glmnet(X.train,Y.train,alpha=1,intercept=F,lambda=lambda.opt)
coef.lasso<- coef(mod.lasso)[-1,1] ##drop intercept





## Look at the scatter plot of ols vs ridge coefficents
data.frame(coef.lasso=coef.lasso,
           coef.lm=coef.lm,
           coef=1:p) %>%
    ggplot()+
    geom_point(aes(coef.lm,coef.lasso),color="red")+
    geom_abline(slope=1,color="blue")


pred.lasso.train <- predict(mod.lasso,newx=X.train)
(mse.lasso.train <- mean((Y.train-pred.lasso.train)^2))

pred.lasso.test <- predict(mod.lasso,newx=X.test)
mse.lasso.test <- mean((Y.test-pred.lasso.test)^2)


## How does this stack up??
## train error rates
c(mse.ols.train,mse.ridge.train,mse.lasso.train)
## test error rates
c(mse.ols.test,mse.ridge.test,mse.lasso.test)



## Final look...
coef.df <- data.frame(coef.lm, coef.ridge=coef.ridge,coef.lasso=coef.lasso)

ggplot(coef.df)+
    geom_point(aes(x=0,y=coef.lm),color="blue",size=1)+
    geom_point(aes(x=1,y=coef.ridge),color="red",size=1)+
    geom_jitter(aes(x=2,y=coef.lasso),color="darkgreen",size=1,width=0)    +
    geom_segment(aes(x=0,xend=1,y=coef.lm,yend=coef.ridge),alpha=0.1)+
    geom_segment(aes(x=1,xend=2,y=coef.ridge,yend=coef.lasso),alpha=0.1)+
    scale_x_continuous("",breaks=NULL)+
    ggtitle("OLS (blue), Ridge (red), and Lasso (green) coefficient estimates")


```


#######################################################

## Assignment
## Reconstruct the ridge or lasso plot for MSE as function of lambda
## directly...that is, do your own 10-fold cross validation
## e.g. plot(cv.ridge)

```{r}
plot(cv.ridge)
plot(cv.lasso)
## Extra credit for adding in the error bars


```

```{r}
numFolds<-10
N<-nrow(X.train)
folds<-sample(1:numFolds,N,rep=T)

```

```{r}
mseKFold<-numeric(numFolds)
for(lam_val in lambda.grid){
  for(fold in 1:numFolds){
    trainx.df  <- X.train[folds != fold,]
    trainy.df <- Y.train[folds != fold,]
    testx.df <- X.train[folds == fold,]
    testy.df <- Y.train[folds == fold,]
    mod.ridge <- 
      glmnet(trainx.df,trainy.df,alpha=0, intercept= FALSE, lambda=lambda.val)
    pred1 <- predict(mod.ridge,newdata=testx.df)  
    mseKFold[fold] <- with(testy.df,mean((testy.df-pred1)^2))
  }
(mse.kfold <- mean(mseKFold))
}
```
#######################################################

## Lasso-> Linear Regression
## Idea: Lasso to identify subset of coefficents and use regular LM on
## tese coefficents
## How many postive values??

```{r}
(lasso.pos <- coef.lasso>0)
sum(lasso.pos)
length(coef.lasso)


## Grab the coefficients
(coeff.pos <- (1:240)[lasso.pos])

X.train0 <- X.train[,coeff.pos]
X.test0 <- X.test[,coeff.pos]
mod.lm0 <- lm(Y.train ~ X.train0)

pred.lm.test0 <- predict(mod.lm0,newdata=list(X.test0))

## How did we do??
mean( (Y.test-pred.lm.test0)^2)

## Not so good.
c(mse.ridge.test,mse.lasso.test)

```