---
title: "3-21-19"
author: "Nathan Kurtz-Enko"
date: "3/21/2019"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,cache = TRUE)
```

#Subset Selection
Given a modeling challenge where there is a large number of
predictors, the principle of parsimony dictates that one should try
to find, among all models of comparable effectiveness, the one with
the smallest number of predictors. This is what is known as subset
selection.

We will use a data set linking US county level demographics to voting
outcomes in the 2012 Presidential election.



##Get started:  Load the libraries
```{r}
library(tidyverse)
library(stringr) ## for string manipulation
library(glmnet)


```


#Buld the data


##County-level election data from 2012
Goal: model county level voter turnout as a function of county
demographics and other factors.

You will need to change this to whereever you keep your data
```{r}
countyOrig.df <- read_csv("~/ADM/class/county_facts.csv")

```


take a peek at the data
```{r}
names(countyOrig.df)
head(countyOrig.df)
summary(countyOrig.df)

```



The field names are a bit cryptic. Use the dictionary of field names.
This makes it easier to read in the original csv file.
```{r}
countyDictionary.df <-
    read.csv("~/ADM/class/county_facts_dictionary.csv")

```


Take a peek...
```{r}
names(countyDictionary.df)
head(countyDictionary.df)


```


County population is PST040214
There is one really small county...
```{r}
with(countyOrig.df,summary(PST045214<10))


```


As is usally the case, we need to clean up the data a bit.
For example, PST040210, PST120214, POP010210 refer to other population estimates.
```{r}
county.df <- countyOrig.df%>%
    ##only keep reasonable sized counties
    filter(PST045214>10)%>%
    ##extra population values
    dplyr::select(-PST040210,-PST120214,-POP010210)%>%
    ##drop out area, we don't care about this
    dplyr::select(-LND110210)%>%
    ##total retail, instead just use per capita value
    dplyr::select(-RTN130207)%>%
    dplyr::select(-area_name) %>% 
    ##Change other total fields to  per capita
    mutate(BZA010213=BZA010213/PST045214,
           BZA110213=BZA110213/PST045214,
           NES010213=NES010213/PST045214,
           SBO001207=SBO001207/PST045214,
           MAN450207=MAN450207/PST045214,
           WTN220207=WTN220207/PST045214,
           AFN120207=AFN120207/PST045214,
           BPS030214=BPS030214/PST045214)


```


Take a peek....we still have 40 fields
```{r}
dim(county.df)
summary(county.df)
head(county.df)
names(county.df)

```


Suppose you want to look up a field description real fast.
Here's one way to look at field descriptions.
```{r}
countyDictionary.df%>%
    filter(column_name=="POP060210")


```


Just to get oriented, 
build a linear  model on everything...select out the descriptor fields

Just for fun.....
```{r}
mod <- lm(POP060210~.,data=county.df)

```


Model summary
```{r}
summary(mod)


```



##County level election results
Add in county-level election results from 2012
```{r}
electOrig.df <-
    read.csv("~/ADM/class/US_elect_county.csv")

head(electOrig.df)


```


Clean up and mutate
```{r}
elect.df <- electOrig.df %>%
    filter(fips>0)%>%
    ##zeros in denominators
    filter(ObamaPerc!="#DIV/0!",
           RomneyPerc!="#DIV/0!")%>%
    ##use stringr functions to clean up numbers
    mutate(ObamaVote=str_replace_all(ObamaVote,",",""),
           RomneyVote=str_replace_all(RomneyVote,",",""))%>%
    ##convert to numbers
    mutate(ObamaVote=as.numeric(as.character(ObamaVote)),
           RomneyVote=as.numeric(as.character(RomneyVote)),
           ObamaPerc=as.numeric(as.character(ObamaPerc)),
           RomneyPerc=as.numeric(as.character(RomneyPerc)),
           TotalVote=RomneyVote+ObamaVote)
head(elect.df)


```


Who won???
```{r}
with(elect.df,sum(ObamaVote)/sum(TotalVote))
with(elect.df,sum(RomneyVote)/sum(TotalVote))



```



##Join county and election data
Now we combine the county demographics with the election
results. We also clean up some more fields.
This gives county level demographics and election outcomes.
Only keep total vote values.
```{r}
names(county.df)
names(elect.df)

countyElect.df <- county.df%>%
    ##right join to ensure we get all the counties
    right_join(elect.df)%>%
    ##get rid of these values
    filter(state_abbreviation !="",
           !is.na(ObamaPerc))%>%
    ##don't need these fields
    dplyr::select(-fips,-state_abbreviation,-State,-County,-ObamaVote,-RomneyVote,-ObamaPerc,-RomneyPerc)%>%
    mutate(VotePerc=TotalVote/PST045214)

head(countyElect.df)
names(countyElect.df)
summary(countyElect.df)


```


The whole shebang model
```{r}
mod <- lm(VotePerc~.,data=countyElect.df)
    

```


what do we have??
```{r}
summary(mod)

```


Lots of variables look interesting..which ones to we keep

Pull off the p-values
```{r}
pvals <- summary(mod)$coefficients[,4]
```


grab the statistically significant values
```{r}
sigPVals <- pvals<0.05

sigFields <- names(countyElect.df)[sigPVals]
sigFields

```


take a look at these...
```{r}
sigDescr <- countyDictionary.df%>%
    filter(column_name %in% sigFields)
```


and here they are
```{r}
sigDescr


```



#Determine subset via cross-validation or bootstrapping.
Use forward stepwise selection (Algorithm 6.2, page 207). Use
#Feature selection: Forward selection
cross-validated (or bootstrapped) prediction error as the quantity
to minimize.

Here's the plan.

 * Start with an empty set of current predictors.
 * For each remaining predictor, add it the current set.
 * Use Cross Validation or bootstrapping to estimate MSE
 *  Keep the set (current + new) which as the smallest MSE.
 *  Repeat with new set current predictors.



#Implementation of Forward Stepwise Selection
First we need  functions that will perform cross validation and
bootstrapping on a data frame.


##In what follows, the number of rows in the data frames will not change.
```{r}
sampleSize <- nrow(countyElect.df)

```


###CrossValidate the model for a given data frame.
```{r}
mseCV <- function(data.df,kfolds=10){
    folds <- sample(1:kfolds,sampleSize,rep=T)
    mse <- rep(0,kfolds)
    for(k in 1:kfolds){
        train.df <- data.df[folds !=k,]
        test.df <- data.df[folds==k,]
        mod <- lm(VotePerc~.,data=train.df)
        vals <- predict(mod,newdata=test.df)
        mse[k] <- with(test.df,mean((VotePerc-vals)^2))
    }
    mean(mse)
}

```


###Bootstrapped version. The default number of boot straps is 50.
```{r}
mseBoot <- function(data.df,M=50){
    mse <- rep(0,M)
    for(m in 1:M){
        bootSamp <- sample(1:sampleSize,sampleSize,rep=T)
        outOfBag <- setdiff(1:sampleSize,bootSamp)
        train.df <- data.df[bootSamp,]
        test.df <-   data.df[outOfBag,]
        mod <- lm(VotePerc~.,data=train.df)
        vals <- predict(mod,newdata=test.df)
        mse[m] <- with(test.df,mean((VotePerc-vals)^2))
    }
    mean(mse)
}


```


###testing...
```{r}
head(countyElect.df)
(numPreds <- length(names(countyElect.df))-1)
(predIDs <- c(1,4,5,6,7,8,9:15))
predIDs <- c(1)
data.df=countyElect.df[,c(predIDs,numPreds+1)]
names(data.df)

mseCV(data.df)
mseBoot(data.df)
mseBoot(data.df,20)

```



##Now build some of the important parts

###All the predictor names and their length.
The last field is the response variable in this case
```{r}
numPreds <- length(names(countyElect.df))-1
```


ALl the predictors (their indices).
```{r}
allPreds <- 1:(numPreds)

```



###Now we are ready. Keep track of the current predictor set and the
"available predictors>
```{r}
currPreds <- c()
```


The predictors we haven't used yet
```{r}
availPreds <- setdiff(allPreds,currPreds)
length(availPreds)

```


The maximimum size of our predictor set. Here we will let it be as
big as possible.
```{r}
maxPreds <- numPreds
```


maxPreds <- 40

All the min MSEs computed along the way
```{r}
minMSE <- numeric(maxPreds)

```



###The main loop...this can take a while. For that reason, I saved the
##data in ElectionPred.Rdata
##load("ElectionPred.Rdata")

```{r}
tot <- 0
while( tot < maxPreds){
    ##add predictor which decreases MSE (as determined by CV or
    ##Bootstrapping)
    ## The MSEs computed as we add each of the available predictors
    allMSE <- numeric(length(availPreds))
    ctr<-1
    for(id in availPreds){
        data.df <- countyElect.df[,c(currPreds,id,numPreds+1)]
        mse <- mseCV(data.df,5)
        ##mse <- mseBoot(data.df,30)
        allMSE[ctr] <- mse
        ctr<-ctr+1
    }
    ##Find the min
    id <- which.min(allMSE)
    ##get the best predictor and MSW
    bestPred <- availPreds[id]
    bestMSE <- min(allMSE)
    ##Add these into the collection
    currPreds <- c(currPreds,bestPred)
    tot <-tot+1
    minMSE[tot] <- bestMSE
    availPreds <- setdiff(allPreds,currPreds)
    ## Print stuff out for debugging and attention-grabbing
    print(sprintf("Predictor Added: %s  MSE Value: %s",bestPred,bestMSE))
    print(currPreds)
}

```


This takes a while, save data
```{r}
save(currPreds,minMSE,file="ElectionPred.Rdata")



```


Take a look. The MSE plot should have the familiar U-shapped look.
```{r}
currPreds
 %>% 
    ggplot()+
    geom_point(aes(pred,minMSE))

```



###Do a better job with the visualizations of  this result...which predictors are having the greatest impact?
#Grap the descriptor names
```{r}
descr <- with(countyDictionary.df,description[currPreds])
```


#Compute the change in MSE (tack 0 on the front (end?))
```{r}
diffMSE <- c(0,minMSE[-length(minMSE)]-minMSE[-1])
head(diffMSE)
```





#Package everything into a data frame

```{r}
result.df <- data.frame(id=1:length(descr),descr,minMSE,diffMSE=-10*diffMSE)

```


#keep the descrs in proper order

```{r}
result.df <- result.df%>%
    mutate(descr=factor(descr,levels=rev(descr)))


```



## Visualizations
A simple plot of the MSE values as predictors are added
```{r}
ggplot(result.df,aes(id,minMSE))+
    geom_point(color="blue")+
    geom_line(color="black",size=0.25)+
    scale_x_continuous("Predictor Number")+
    scale_y_continuous("MSE")+
    ggtitle("Subset Selection: MSE Reduction")

```



##We can see a decrease in impact at around the first 15 or so predictors.

A different look
```{r}
ggplot(result.df,
       aes(descr,minMSE))+
    geom_bar(stat="identity",fill="blue")+
    coord_flip()+
    ggtitle("Subset Selection: MSE Decrease")

```


Or this one....Lollipop plot
```{r}
ggplot(result.df)+
    geom_segment(aes(x=id,xend=id,y=0,yend=minMSE),color="blue")+
    geom_point(aes(x=id,y=minMSE),color="blue",size=2)+
    scale_x_continuous(breaks=1:length(descr),label=descr)+
    coord_flip()+
    ggtitle("Subset Selection: MSE Decrease")

```


Same for the difference in MSE
```{r}
ggplot(result.df,aes(descr,diffMSE))+
    geom_bar(stat="identity",fill="red")+
    coord_flip()+
    ggtitle("Subset Selection: MSE Chage")

```


Lollipop plot
```{r}
ggplot(result.df)+
    geom_segment(aes(x=id,xend=id,y=0,yend=diffMSE),color="red")+
    geom_point(aes(x=id,y=diffMSE),color="red",size=2)+
    scale_x_continuous(breaks=1:length(descr),label=descr)+
    coord_flip()+
    ggtitle("Subset Selection: MSE Decrease")


```



##Being parsimonious.
From this it looks as if about the first 25 or so predictors that
we added do the most of the work. Which
```{r}
cut <- 10
idPreds <- currPreds[1:cut]
head(countyDictionary.df)
mseFields <- with(countyDictionary.df,column_name[currPreds[1:25]])


```


How does this compare with the selection fields with significant
p-values?
Overlap
```{r}
sameFlds <- intersect(mseFields,sigFields)
countyDictionary.df%>%
    filter(column_name %in% sameFlds)



mseOnly <- setdiff(mseFields,sigFields)
countyDictionary.df%>%
    filter(column_name %in% mseOnly)

sigOnly <- setdiff(sigFields,mseFields)
countyDictionary.df%>%
    filter(column_name %in% sigOnly)




```



###One last look...put minMSE and diffMSE on back-to-back bar chart
```{r}
result.df2 <- result.df%>%
    gather(type,val,minMSE:diffMSE)

```


###Back-to-back MSE and differences
```{r}
ggplot(result.df2 %>%filter(type == "minMSE"| val<0) ,
       aes(descr,val,group=type,fill=type))+
    geom_bar(stat="identity",width=0.5)+
    scale_fill_manual(values=c("red","blue"))+
    scale_y_continuous("",breaks=seq(-0.06,0.2,by=.02),label=abs(seq(-0.06,0.2,by=.02)/10))+
    scale_y_continuous("",breaks=seq(-0.06,0.2,by=.01),label=abs(seq(-0.06,0.2,by=.01)/10))+
    theme(axis.text.x = element_text(angle = 45, hjust = 1,size=8))+
    coord_flip()+
    ggtitle("Subset Selection: Election 2012\nMSE and MSE reduction")

```


##Or with Lollipop plot

```{r}
ggplot(result.df)+
    geom_segment(aes(x=1,xend=id,y=0,yend=0),size=.1,color="black")+
    geom_segment(aes(x=id,xend=id,y=0,yend=minMSE),color="blue")+
    geom_point(aes(x=id,y=minMSE),color="blue",size=1.5)+
    geom_segment(aes(x=id,xend=id,y=0,yend=diffMSE),color="red")+
    geom_point(aes(x=id,y=diffMSE),color="red",size=1.5)+
    scale_x_continuous(breaks=1:length(descr),label=descr)+
    coord_flip()+
    ggtitle("Subset Selection: MSE Decrease")

```



##Only show the top 25 or so
```{r}
cut <- 25
ggplot(result.df2 %>%
       filter(type == "minMSE"| val<0)%>%
      filter(descr %in% descr[1:cut]),
       aes(descr,val,group=type,fill=type))+
    geom_bar(stat="identity")+
    scale_fill_manual(values=c("red","blue"))+
    scale_y_continuous("",breaks=seq(-0.06,0.2,by=.01),label=abs(seq(-0.06,0.2,by=.01)/10))+
    theme(axis.text.x = element_text(angle = 45, hjust = 1,size=6))+
    coord_flip()+
    ggtitle("Subset Selection: Election 2012\nMSE and MSE reduction (top 25)")

```


##Or with Lollipop plot
```{r}
ggplot(result.df %>%
       filter(descr %in% descr[1:cut]))+
    geom_segment(aes(x=1,xend=id,y=0,yend=0),size=.1,color="black")+
    geom_segment(aes(x=id,xend=id,y=0,yend=minMSE),color="blue")+
    geom_point(aes(x=id,y=minMSE),color="blue",size=1.5)+
    geom_segment(aes(x=id,xend=id,y=0,yend=diffMSE),color="red")+
    geom_point(aes(x=id,y=diffMSE),color="red",size=1.5)+
    scale_x_continuous(breaks=1:length(descr),label=descr)+
    coord_flip()+
    ggtitle("Subset Selection: MSE Decrease")


```


##What does Lasso say here? Compare this list of predictors with the
results from lasso. Is there any agreement as to the order of
predictors? For example, what are last 15 predictors in lasso (as
lambda increases).Are these related to the first 15 predictors
selected via this method?

```{r}
X.train <- as.matrix(dplyr::select(countyElect.df, -VotePerc))
Y.train <- as.matrix(dplyr::select(countyElect.df, VotePerc))
lambda.grid <- 10^seq(-3,1,length=100)
```

```{r}
cv.lasso <- cv.glmnet(X.train,Y.train,alpha=1,intercept=F,lambda=lambda.grid)
```

```{r}
sampleSize <- nrow(countyElect.df)

```


```{r}
mseCV <- function(data.df,kfolds=10){
    folds <- sample(1:kfolds,sampleSize,rep=T)
    mse <- rep(0,kfolds)
    for(k in 1:kfolds){
        train.df <- data.df[folds !=k,]
        test.df <- data.df[folds==k,]
        mod.lasso <- glmnet(X.train,Y.train,alpha=1,intercept=F,lambda=.1)
        pred.lasso.train <- predict(mod.lasso,newx=X.train)
        mse[k] <- with(test.df,mean((VotePerc-vals)^2))
    }
    mean(mse)
}

```



```{r}
mseBoot <- function(data.df,M=50){
    mse <- rep(0,M)
    for(m in 1:M){
        bootSamp <- sample(1:sampleSize,sampleSize,rep=T)
        outOfBag <- setdiff(1:sampleSize,bootSamp)
        train.df <- data.df[bootSamp,]
        test.df <-   data.df[outOfBag,]
        mod.lasso <- glmnet(X.train,Y.train,alpha=1,intercept=F,lambda=.1)
        pred.lasso.train <- predict(mod.lasso,newx=X.train)
        mse[m] <- with(test.df,mean((VotePerc-vals)^2))
    }
    mean(mse)
}


```


#Assignment 

Adapt this method to use a backward stepwise selection
(Algorithm 6.3).

#Assinment 2
Use the data set Prostate.csv whose response variable is ldsa. The
predictors are all the other fields. Use both forward and backward
step selection (with mseBoot) to determine an optimal predictor
set. Do the sets differ? Provide supporting graphics which illustrate
the results of your exploration. Compare to lasso's identification
of the most important predictors

#Assignment 3
Adapt to classification scenario in which error rate is the the
quantity to minize. Use either Forward (preferred) or Backward
selection. Find a data set to apply your method on (see UCI Machine
Learning Datasets.
## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
