---
title: "homework_6"
author: "Nathan Kurtz-Enko"
date: "3/4/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(broom)
library(dplyr)
library(FNN)
```

## Assignment
Explore and discuss the efficacy of KNN vs Linear Regression using the framework above. In particular, discuss the relationship between
these two methods as:

* The number of predictors (p) grows
* The underlying model is linear vs non-linear.
* The size of the data sets changes

I provide you with some examples of underlying functions, especially nonlinear functions. It is recommended that you create at least one non-linear function of your own to use in your analysis.

Key question: Are there situations when you would recommend using KNN vs LM (or vice versa)? 

Write this up in a self-contained RMarkdown document. 


```{r}
#how many predictors
p <- 2:5
p_act <- sample(p, 1)
p_act
```

```{r}
#A vector to help quickly create our linear function
dotP <- sample(-10:10,p,rep=T)

#lets define some functions for modeling
f1 <- function(x){
  sum(dotP*x)
}
f2 <- function(x){
  11+sum(x^2*dotP)
}
f3 <- function(x){
  42+sum(x^3*dotP)
}
```

```{r, warning=FALSE}
#lets choose which function to use
our_func <- str_c("f", sample(1:3, 1))
our_func
```
```{r, warning=FALSE}
#how many points in the data
n <- 5000

#lets create some training data then
train.xi <- matrix(runif(p_act*n, 0, 7), ncol = p_act) #for the max and min here, choose any numbers, it doesn't really matter
train.y <- apply(train.xi, 1, our_func) + rnorm(n, 0, 1)
```

```{r}
#lets do knn predictions just to test some stuff
### Getting loose on the knn. Predict the training data
kval<-7 #choose any random number here for now
mod.knn<-knn.reg(train.xi,train.xi,train.y,k=kval)
pred.knn<-mod.knn$pred
##How'd we do?
(err.knn<-mean((train.y-pred.knn)^2))
```

```{r}
#lets create some testing data
test.xi <- matrix(runif(p_act*n, 0, 7), ncol = p_act)

#lets make predictions on our test data using training data
test.knn <- knn.reg(train.xi, test.xi, train.y, k = kval)
test.pred.knn <- test.knn$pred

#how is the test error
test.err.knn <- mean((train.y-test.pred.knn)^2)
test.err.knn
```

```{r}
#lets test over a bunch of kvals
##Look at a bunch of values of kval.
maxK<-100
kval_range <- 1:maxK
errs<-tibble(kval = kval_range, error = kval_range)
for(k in kval_range){
  mod.knn <- knn.reg(train.xi, test.xi, train.y, k = k)
  errs[k,2]<-mean((train.y-mod.knn$pred)^2)
}
min_knn_err <- min(errs$error)
best_k <- (errs%>%filter(error == min_knn_err))$kval

```

```{r}
#lets find the error with our best value of k
test.best.val.k <- knn.reg(train.xi, test.xi, train.y, k = best_k)
test.best.val.k.pred <- test.best.val.k$pred

test.best.val.k.err <- mean((train.y-test.best.val.k.pred)^2)
test.best.val.k.err #this is still pretty bad
```

```{r}
#Now lets start doing some linear modeling stuff
#first put things into a data frame so all info can be accessed when using lm()
train.df<-data.frame(train.xi,y=train.y)
##fix the names
names(train.df)[1:p_act]<-paste0("X",1:p_act)
#next make the model and make predictions
lin.mod <- lm(y~., data = train.df)
predictions.lin.mod <- predict(lin.mod)

#lets find our error
train.lin.mod.err <- mean((train.y-predictions.lin.mod)^2)
train.lin.mod.err
```

```{r}
#lets do this with our test data
test.df <- data.frame(test.xi)
names(test.df)[1:p_act] <- paste0("X", 1:p_act)

test.pred.lin.mod <- predict(lin.mod, newdata = test.df)

test.lin.mod.err <- mean((train.y-test.pred.lin.mod)^2)
test.lin.mod.err # this is not good
```

```{r, warning = FALSE}
#lets do all of this for a variety of input values and record the errors into a tibble and plot
err_for_lm_KNN <- tibble(value_p = 1:15, err_lm = 1:15, err_knn = 1:15)
for(i in 1:15){
  train.xi <- matrix(runif(i*n, 0, 7), ncol = i) #for the max and min here, choose any numbers, it doesn't really matter
  train.y <- apply(train.xi, 1, our_func) + rnorm(n, 0, .5)
  train.df <- data.frame(train.xi, y = train.y)
  names(train.df)[1:i] <- paste0("X", 1:i)
  test.xi <- matrix(runif(i*n, 0,7), ncol = i)
  test.df <- data.frame(test.xi)
  names(test.df)[1:i] <- paste0("X", 1:i)
  maxK<-100
  kval_range <- 1:maxK
  errs<-numeric(maxK)
  for(k in kval_range){
    mod.knn<-knn.reg(test.xi,train.xi,train.y,k=k)
    errs[k]<-mean((train.y-mod.knn$pred)^2)
  }
  best_k_index <- which.min(errs)
  best_k <- kval_range[best_k_index]
  test.best.val.k <- knn.reg(train.xi, test.xi, train.y, k = best_k)
  test.best.val.k.pred <- test.best.val.k$pred
  test.best.val.k.err <- mean((train.y-test.best.val.k.pred)^2)
  err_for_lm_KNN[i,2] = test.best.val.k.err
  lin.mod <- lm(y~., data = train.df)
  test.pred.lin.mod <- predict(lin.mod, newdata = test.df)
  test.lin.mod.err <- mean((train.y-test.pred.lin.mod)^2)
  err_for_lm_KNN[i,3] = test.lin.mod.err
  print(c("inputs done:", i))
}
```

```{r}
#lets plot this
ggplot(data = err_for_lm_KNN)+
  geom_point(aes(x = value_p, y = err_lm), color = "red")+
  geom_line(aes(x = value_p, y = err_lm), color = "red")+
  geom_point(aes(x = value_p, y = err_knn), color = "blue")+
  geom_line(aes(x = value_p, y = err_knn), color = "blue")+
  ggtitle("MSE for input values: red = lm, blue = knn")+
  ylab("mse")+
  xlab("number of inputs")
```

```{r}
min_knn <- min((err_for_lm_KNN%>%filter(value_p == 1))$err_knn)
min_lm <- min((err_for_lm_KNN%>%filter(value_p == 1))$err_lm)
min_knn 
min_lm
```


Generally, it appears that KNN is slightly less effective of a method for a variety of input values when the data is fairly small (100 points). However, as the size of the dataset grows (greater than 10000 points), when there are fewer inputs, the KNN regression method can be slightly more effective. So, it would seem that for very large datasets with only a few inputs, perhpas 3 or less, KNN may be a better method than lm for accurate predictions. 