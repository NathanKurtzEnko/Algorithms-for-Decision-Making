---
title: "homework 2"
author: "Nathan Kurtz-Enko"
output: pdf_document
---

#Assignment
Redo this process with different underlying data models. In other
words, alter how the model above was defined to generate the
data. In the example above, I used two normal distributions, two
for each of the classes. They had different means, but the same
variances. Also, both classes had the same number of data points.

Possible modifications include replacing the normal distribution
with, say, the uniform.

```{r, include=FALSE}
library(tidyverse)
```


```{r}
###build data

#define means
c1 <- c(-1,0)
c2 <- c(-2,1)

#define standard deviations
s1 <- .7 
s2 <- .7

#define numbers of data points
m1 <- 100
m2 <- 100

#define classes
class <- c("A", "B")

#define vectors of randomly generated normal distribution data points
v1 <- rnorm(m1, c1[1], s1)
v2 <- rnorm(m1, c1[2], s1)
v3 <- rnorm(m2, c2[1], s2)
v4 <- rnorm(m2, c2[2], s2)

#sample vectors
x1 <- sample(c(v1,v2), m1, rep=F)
x2 <- sample(c(v3,v4), m2, rep=F)
```

```{r}
#create tibble or data frame
training_data <- tibble(value = c(x1,x2), class = factor(c(rep("A", m1), rep("B", m2))))
```

```{r}
#make plot
x_values <- seq(-4,4,.01)
a_class <- training_data %>%
  filter(class == "A")
b_class <- training_data %>%
  filter(class == "B")
ggplot()+
  geom_line(aes(x = x_values, y = dnorm(x_values, c1[1], s1)), color = "red")+
  geom_line(aes(x = x_values, y = dnorm(x_values, c1[2], s1)), color = "red")+
  geom_line(aes(x = x_values, y = dnorm(x_values, c2[1], s2)), color = "blue")+
  geom_line(aes(x = x_values, y = dnorm(x_values, c2[2], s2)), color = "blue")+
  geom_point(data = a_class, aes(x = value, y = 0), color = "red")+
  geom_point(data = b_class, aes(x = value, y = -.2), color = "blue")+
  ggtitle("Distribution", subtitle = "class a = red, class b = blue")
```

```{r}
###determine optimal value of K

#best error
bayes_probability_class_a<-function(x){
  ##Prob (unnormalized) of being in class A
  p1<-sum(dnorm(x,c1,s1))
  ##Prob (unnormalized) of being in class B
  p2<-sum(dnorm(x,c2,s2))
  p1/(p1+p2)
}
```

```{r}
probability_class_a <- map_dbl(training_data$value, bayes_probability_class_a)

training_data_error_calc <- training_data %>%
  mutate(bayes_prediction_class_a = probability_class_a > .5,
         bayes_prediction_class = ifelse(bayes_prediction_class_a, "A", "B"))

bayes_error <- mean(!(training_data_error_calc$class)==(training_data_error_calc$bayes_prediction_class))
```

```{r}
#plot
bayes_class_a <- training_data_error_calc %>%
  filter(bayes_prediction_class == "A")
bayes_class_b <- training_data_error_calc %>%
  filter(bayes_prediction_class == "B")
ggplot()+
  geom_line(aes(x = x_values, y = dnorm(x_values, c1[1], s1)), color = "red")+
  geom_line(aes(x = x_values, y = dnorm(x_values, c1[2], s1)), color = "red")+
  geom_line(aes(x = x_values, y = dnorm(x_values, c2[1], s2)), color = "blue")+
  geom_line(aes(x = x_values, y = dnorm(x_values, c2[2], s2)), color = "blue")+
  geom_point(data = a_class, aes(x = value, y = 0), color = "red")+
  geom_point(data = b_class, aes(x = value, y = -.2), color = "blue")+
  ggtitle("Distribution", subtitle = "class a = red, class b = blue")+
  geom_point(data = bayes_class_a, aes(x = value, y = -.4), color = "red")+
  geom_point(data = bayes_class_b, aes(x = value, y = -.6), color = "blue")

```

```{r}
#define range of values for K
k <- 1:20

#define test point
x0 <- .2

#define distance function
distance <- function(x_i, x_f){
  abs(x_f-x_i)
}
```

```{r}
#find and sort distances of all data points from test point
training_data3 <- training_data_error_calc %>%
  mutate( `dist from x0` = map_dbl(training_data_error_calc$value, function(x) distance(x0, x)),
          id = 1:n())%>%
    arrange(`dist from x0`)

#knn values
knn_vals <- training_data3 %>%
  slice(k)
```

```{r}
#plot
ggplot(data = training_data3)+
  geom_point(aes(x = id, y = value, color=id %in% knn_vals$id))+
  guides(color=F)+
  geom_hline(yintercept=x0,color="orange",size=1)
```

```{r}

#made prediction
myKnn<-function(pt,kval,train_data,allResp=FALSE){
  ##make sure kNear isn't too big
  kval<-min(nrow(train_data),kval)
  ##identify the classes
  classes<- with(train_data, as.character(unique(class)))
  xVals<-with(train_data,value)
  tot<-length(xVals)
  ##get the ascending order of distances
  allOrds<-order(map_dbl(xVals,function(x) distance(x,pt)))
  ##get the k closest
  closest<-allOrds[1:kval]
  ##Decide which class wins..not the most elegant ending.
  ## But I haven't seen an easier way 
  ##extract the classes of the the closest
  ##Take the mean equal to classes[1]
  p<-with(train_data[closest,],mean(class==classes[1]))
  if(p>0.5){
    res<-classes[1]
  }else{
    res<-classes[2]
  }
  ##Check how much output to return
  if(allResp){
    ## a list of resp, probability, and the indices of the nhbs.
    list(res,p,(1:tot)[closest])
  } else{
    res
  }
}

#extend prediction function to work over all data
myKnn2 <- function(test_data,kval,train_data){
   with(test_data,
      ##send each x value in the data frame 
      ## to myKnn.
      ##map over the x values in test.df. 
      ##map_chr because the returns are "A"/"B"
     map_chr(value,
             function(x1) myKnn(x1,kval,train_data)))
}
```

```{r}
#check performance
k1 <- 5

knn_pred1 <- myKnn2(training_data3, k1, training_data3)
training_data_pred <- training_data3 %>%
  mutate(knn_prediction = knn_pred1)
  
knn_class_a <- training_data_pred %>%
  filter(knn_prediction == "A")
knn_class_b <- training_data_pred %>%
  filter(knn_prediction == "B")
ggplot()+
  geom_line(aes(x = x_values, y = dnorm(x_values, c1[1], s1)), color = "red")+
  geom_line(aes(x = x_values, y = dnorm(x_values, c1[2], s1)), color = "red")+
  geom_line(aes(x = x_values, y = dnorm(x_values, c2[1], s2)), color = "blue")+
  geom_line(aes(x = x_values, y = dnorm(x_values, c2[2], s2)), color = "blue")+
  geom_point(data = a_class, aes(x = value, y = 0), color = "red")+
  geom_point(data = b_class, aes(x = value, y = -.2), color = "blue")+
  ggtitle("Distribution", subtitle = "class a = red, class b = blue")+
  geom_point(data = bayes_class_a, aes(x = value, y = -.4), color = "red")+
  geom_point(data = bayes_class_b, aes(x = value, y = -.6), color = "blue")+
  geom_point(data = knn_class_a, aes(x = value, y = -.4), color = "red")+
  geom_point(data = knn_class_b, aes(x = value, y = -.6), color = "blue") 
```

```{r}
#create new tibble with errors
error_rate_for_knn <- tibble(`num_nearest_neighbor` = k, error = k)

#loop through data with different values for K and find errors and store in '
for(i in k){
  knn_pred <- myKnn2(training_data3, i, training_data3)
  training_data4 <- training_data3 %>%
    mutate(knn_prediction = knn_pred)
  error_rate_for_knn[i, 1] = i
  error_rate_for_knn[i, 2] = mean(!(training_data4$class)==(training_data4$knn_prediction))
}
```

```{r}
buildData<-function(N, mu1, sig1, mu2, sig2){
  #print(N)
  v1<-rnorm(N,mu1[1],sig1)
  v2<-rnorm(N,mu1[2],sig1)
  x1<-sample(c(v1,v2),N,rep=F)
  v1<-rnorm(N,mu2[1],sig2)
  v2<-rnorm(N,mu2[2],sig2)
  x2<-sample(c(v1,v2),N,rep=F)
  #print(c(x1,x2))
  data.frame(value=c(x1,x2),
                      class=rep(c("A","B"),each=N))
}

#define number of test data points
m3 <- 100

#build data
test_data <- buildData(m3, c1, s1, c2, s2)

#test
test_predictions <- myKnn2(test_data, 5, training_data)
test_data2 <- test_data %>%
  mutate(test_pred_knn = test_predictions)
```

```{r}
#create new tibble with errors
test_error_rate_for_knn <- tibble(`num_nearest_neighbor` = k, error = k)

#loop through data with different values for K and find errors and store in '
for(i in k){
  test_knn_pred <- myKnn2(test_data, i, training_data3)
  test_data3 <- test_data %>%
    mutate(test_knn_prediction = test_knn_pred)
  test_error_rate_for_knn[i, 1] = i
  test_error_rate_for_knn[i, 2] = mean(!(test_data3$class)==(test_data3$test_knn_prediction))
}
```

```{r}
myKnnReg<-function(x0,kNear,train.df,allResp=FALSE){
  ##make sure kNear isn't too big
  kNear<-min(nrow(train.df),kNear)
  xVals<-with(train.df,value)
  tot<-length(xVals)
  ##get the ascending order of distances
  allOrds<-order(map_dbl(xVals,function(x) distance(x,x0)))
  ##get the k closest
  closest<-allOrds[1:kNear]
  ##Take the mean of the nearest responses
  res<-with(train.df[closest,],mean(resp))
  if(allResp){
    list(res,p,(1:tot)[closest])
  } else{
    res
  }
}

myKnnReg2 <- function(test.df,kval,train.df){
   with(test.df,
        ##send each x value in the data frame 
        ## to myKnn.
     map_dbl(value,
             function(x1) myKnnReg(x1,kval,train.df)))
}

```

```{r}
#build training data for myKnnReg and test data
f<-function(x) x+ sin(5*pi*x)
N <-200 # number of data points
sig<-.5 # for the noise
value<-runif(N,0,1) # inputs
resp<-f(value)+rnorm(N,0,sig) #realized values
train.df<-data.frame(value,resp)

x2<-runif(N,0,1)
resp2<- f(x2)+rnorm(N,0,sig)
test.df <- data.frame(value = x2,resp = resp2)
```

```{r}
poly <- function(deg){
  formula <- "y~I(x)"
  while(deg > 1){
    formula <- str_c(formula, sprintf("I(x^%s)", deg), sep = "+")
    deg <- deg -1
  }
  formula
}

predictions <- function(N, X, deg){
  x <- runif(N, 0, 1)
  y <- f(x) + rnorm(N, 0, .5)
  model <- lm(as.formula(poly(deg)))
  predict(model, newdata = data.frame(x = X))
}
```

```{r}
train_error_rate_for_knnReg <- tibble(`num_nearest_neighbor` = k, error = k)

#loop through data with different values for K and find errors and store in '
for(i in k){
  train_knn_pred_reg <- myKnnReg2(train.df, i, train.df)
  train.df2 <- train.df %>%
    mutate(train_knn_prediction = train_knn_pred_reg)
  train_error_rate_for_knnReg[i, 1] = i
  train_error_rate_for_knnReg[i, 2] = mean(((train.df2$train_knn_prediction)-(train.df2$resp))^2)
}
```

```{r}
#calculate and store test error
test_error_rate_for_knnReg <- tibble(`num_nearest_neighbor` = k, error = k)
for(i in k){
  test_knn_pred_reg <- myKnnReg2(test.df, i, train.df)
  test.df2 <- test.df %>%
    mutate(test_knn_prediction = test_knn_pred_reg)
  test_error_rate_for_knnReg[i,1] = i
  test_error_rate_for_knnReg[i, 2] = mean(((test.df2$test_knn_prediction)-(test.df2$resp))^2)
}
```

```{r}
#use linear regression to find error
x0 <- .5
y0 <- f(x0) + rnorm(N, 0, sig)
test_error_rate_for_linReg <- tibble(`polynomial_deg` = k, error = k)
for(i in k){
  y0_hat <-  map_dbl(1:100, function(x) predictions(N, x0, i))
  test_error_rate_for_linReg[i, 1] = i
  test_error_rate_for_linReg[i, 2] = mean((y0_hat-y0)^2)
}
```

```{r}
#plot error for knnReg
ggplot(data = test_error_rate_for_knnReg, aes(x = `num_nearest_neighbor`, y = error))+
  geom_point()+
  geom_line()+
  geom_vline(xintercept = (test_error_rate_for_knnReg%>%filter(error == min(error)))$`num_nearest_neighbor`)
```

```{r}
#plot error for linReg
ggplot(data = test_error_rate_for_linReg, aes(x = `polynomial_deg`, y = error))+
  geom_point()+
  geom_line()+
  geom_vline(xintercept = (test_error_rate_for_linReg%>%filter(error == min(error)))$`polynomial_deg`)
```

Both of these methods have a fairly similar error so perhaps either method could be used.