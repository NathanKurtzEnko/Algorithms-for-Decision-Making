---
title: "homework 16"
author: "Nathan Kurtz-Enko"
date: "4/16/2019"
output: pdf_document
---

```{r include=FALSE}
library(tidyverse)
library(MASS)

library(rpart)
```

#Assignment: 
Ada Boosting
Boosting also works with classiﬁcation models. In this case, MSE is replaced by error rate. Again, a weak learner (a shallow tree) is the used at each stage. However, the means of “going after” the problems has to be adjusted. Here’s the idea behind AdaBoost. Supposethedatasethas N observations x1,x2,...,xn andresponse y1,...,y2. Forthealgorithm’spurposes, assume that yi =±1. A crucial part of the AdaBoost algorithm is how we weight the observations. Up to this point, our classiﬁers have assumed all the observations are on equal footing. However, most classiﬁcation algoriths (trees, included) allow a weight to be placed on each observation. In this way, the error rate now places more of an emphasis on some observations than others. Here is a summary of AdaBoosting. It follows the description given in CASI, Section 17.4, page 343.
6
Start with a set of weights wi = 1, i = 1,2,...,N. Usually, all the wi are equal at the start (but they don’t have to be). Boost a total of M times. For each m =1,2,...,M, do the following.

• Fit the classiﬁer Gm(x) to the training data using weights wi. (Note: this means using the weights option in rpart. See documentation). 

• Compute the weighted error. errm =PN i=1 wiI (yi 6= Gm(xi)) PN i=1 wi 

• Compute αm =log1−errm errm  

• Update the weights via: wi ← wi exp[αmI (yi 6= Gm(xi))] 

• Repeat

The ﬁnal outputed model is

G(x)= sgn"M X m=1
αmGm(x)#. where sgn(x) means return ±1 depending on the sign of the number x. What is going on here? Think about this algorithm. What is the logic behind be the choice of the weights and the value of αm? Why does this makes sense, especially in light of the logic behind the boosting for quantitative data.
Implementation Build your own AdaBoost algorithm. Do so in the spirit of how we built the regression boosting algorithm fromclasslastweek. Thestructureisaboutthesame, there M boostingsteps, weakclassiﬁers(usingrpart), and a ﬁnal model consisting of the sum of the weak learner models built along the way. As part of your implementation, build your own synthetic data set. Make it so that it has at least two predictors. Try to arrange your data set so that it isn’t a simple logistic regression sort of data set. When you have a working version of AdaBoost, build it on some training data and evaluate it on some training data. Play around with diﬀerent values of M, perhaps even cross-validate on your data to ﬁnd the best value. Or, just use train+test combinations. Compare its performance to other algorithms such as KNN, Logistic (with shrinkage), and Random Forests. How well does yours work? How well does it work compared to the R gbm with AdaBoost? Once you have it working, try to apply it to the Spam data. Compare your AdaBoost to R’s version. In each case Use the SPAM data set from a couple classes ago as your test data set.

##build data 

```{r}
numMeans <- 4
sd<- 3*sqrt(1/5)
mu0<-cbind(rnorm(numMeans,1,sd),rnorm(numMeans,1,sd))
mu1<-cbind(rnorm(numMeans,-1,sd),rnorm(numMeans,-1,sd))
buildData<-function(N){
  rndMean <- sample(1:numMeans,N,rep=T)
  vals0<- t(apply(mu0[rndMean,],1,function(mu) mvrnorm(1,mu,diag(2)*sd)))
  rndMean <- sample(1:numMeans,N,rep=T)
  vals1 <- t(apply(mu1[rndMean,],1,function(mu) mvrnorm(1,mu,diag(2)*sd)))
  vals <- rbind(vals1,vals0)
  data.frame(row=1:(2*N),x=vals[,1],y=vals[,2],class=rep(c("A","B"),each=N))
}
```

```{r}
N<-500
train.df <-buildData(N)
test.df <-buildData(N)
```

##Begin algorithm

###initialize

```{r}
# n = 2* N
Weights <- numeric(2*N)+(1/(2*N))
```

##choose B  and update weights
```{r}
#let B = 4, because why not
b <- 1:4
```

###classify training data set
```{r, include = FALSE}
#use rpart as our classifier

Weights.tb <- tibble(`C_b(x)` = Weights*0,
                     weights = Weights)
classifier <- tibble(`C_b(x)` = b,
                 err_b = b,
                 alpha_b = b)
for(B in b){
  mod.adaBoost <- rpart(class ~ x + y, 
                        data = train.df,
                        weights = Weights,
                        method = "class")
  pred <- predict(mod.adaBoost, newdata = test.df, type = "class")
  err_b = sum(Weights * (pred != test.df$class))/sum(Weights)
  alpha_b = log((1-err_b)/err_b)
  for(i in 1:length(Weights)){
    Weights[i] <- Weights[i] * exp(alpha_b * (pred[i] != test.df$class[i]))
    Weights.tb[i, 2] = Weights[i]
  }
  #Weights.tb <- rbind(Weights.tb, 
   #                   tibble(`C_b(x)` = (Weights*0)+B, weights = Weights))
  classifier[B, 2] <- err_b
  classifier[B, 3] <- alpha_b
}
```

```{r}
New_weights <- Weights.tb$weights

mod.adaBoost <- rpart(class~ x+y,
                      data = train.df,
                      weights = New_weights,
                      method = "class")
pred <- predict(mod.adaBoost, newdata = test.df, type = "class")
err <- mean(pred != test.df$class)
err
```


